import asyncio
import logging
from datetime import datetime
from typing import Optional
import aiohttp
from bs4 import BeautifulSoup
from telegram import Update
from telegram.ext import Application, MessageHandler, CommandHandler, filters, ContextTypes
from supabase import create_client, Client
import re

# ============ –ù–ê–°–¢–†–û–ô–ö–ò ============
TELEGRAM_TOKEN = "8540569762:AAFnvJS9v7P6mlfhK1sfGSpQ_nIsY2bbM6s"
SUPABASE_URL = "https://dqpqhvaikapnnmablvxh.supabase.co"
SUPABASE_KEY = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImRxcHFodmFpa2Fwbm5tYWJsdnhoIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc2OTQ5MzE3MywiZXhwIjoyMDg1MDY5MTczfQ.bNsJE5orouvDoCHa4q9p0FwbaMDgsLuhXSIGmN9h7Qc"
CHANNEL_ID = "@iqsafety_news"

# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

# Supabase –∫–ª–∏–µ–Ω—Ç
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# ============ –ö–õ–Æ–ß–ï–í–´–ï –°–õ–û–í–ê –î–õ–Ø –§–ò–õ–¨–¢–†–ê–¶–ò–ò ============
KEYWORDS = [
    # –í–∏–¥–µ–æ–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ
    r'–≤–∏–¥–µ–æ–Ω–∞–±–ª—é–¥–µ–Ω', r'–∫–∞–º–µ—Ä', r'cctv', r'ip-–∫–∞–º–µ—Ä', r'–≤–∏–¥–µ–æ—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ç–æ—Ä',
    r'nvr', r'dvr', r'–≤–∏–¥–µ–æ–∞–Ω–∞–ª–∏—Ç–∏–∫',
    
    # –°–ö–£–î
    r'—Å–∫—É–¥', r'–∫–æ–Ω—Ç—Ä–æ–ª—å –¥–æ—Å—Ç—É–ø', r'—Ç—É—Ä–Ω–∏–∫–µ—Ç', r'—à–ª–∞–≥–±–∞—É–º', r'–¥–æ–º–æ—Ñ–æ–Ω',
    r'–≤–∏–¥–µ–æ–¥–æ–º–æ—Ñ–æ–Ω', r'—Å—á–∏—Ç—ã–≤–∞—Ç–µ–ª—å', r'–∫–∞—Ä—Ç-—Ä–∏–¥–µ—Ä', r'—ç–ª–µ–∫—Ç—Ä–æ–∑–∞–º–æ–∫',
    
    # –ü–æ–∂–∞—Ä–Ω–∞—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å
    r'–ø–æ–∂–∞—Ä–Ω', r'–æ–≥–Ω–µ—Ç—É—à–∏—Ç–µ–ª', r'–¥–∞—Ç—á–∏–∫ –¥—ã–º', r'–ø–æ–∂–∞—Ä–æ—Ç—É—à–µ–Ω',
    r'—Å–ø—Ä–∏–Ω–∫–ª–µ—Ä', r'–æ–ø—Å', r'–∞—É–ø—Ç', r'–ø–æ–∂–∞—Ä–Ω–∞—è —Å–∏–≥–Ω–∞–ª–∏–∑–∞—Ü',
    
    # –û—Ö—Ä–∞–Ω–Ω–∞—è —Å–∏–≥–Ω–∞–ª–∏–∑–∞—Ü–∏—è
    r'—Å–∏–≥–Ω–∞–ª–∏–∑–∞—Ü', r'–æ—Ö—Ä–∞–Ω', r'–¥–∞—Ç—á–∏–∫ –¥–≤–∏–∂–µ–Ω', r'–¥–∞—Ç—á–∏–∫ —Ä–∞–∑–±–∏—Ç',
    r'–ø–µ—Ä–∏–º–µ—Ç—Ä', r'–æ–≥—Ä–∞–∂–¥–µ–Ω–∏–µ', r'—Ç—Ä–µ–≤–æ–∂–Ω',
    
    # –ë–∏–æ–º–µ—Ç—Ä–∏—è
    r'–±–∏–æ–º–µ—Ç—Ä', r'—Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –ª–∏—Ü', r'–æ—Ç–ø–µ—á–∞—Ç–æ–∫', r'—Å–∫–∞–Ω–µ—Ä –ª–∏—Ü',
    r'face recognition', r'–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü', r'–∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü',
    
    # –ë—Ä–µ–Ω–¥—ã –º–∏—Ä–æ–≤—ã–µ
    r'hikvision', r'dahua', r'axis', r'bosch', r'siemens', r'hanwha',
    r'honeywell', r'hochiki', r'schneider electric', r'panasonic',
    
    # –ë—Ä–µ–Ω–¥—ã —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ/–°–ù–ì
    r'–±–æ–ª–∏–¥', r'—Ä—É–±–µ–∂', r'perco', r'parsec', r'–æ—Ä–∏–æ–Ω', r'itv', r'—Å–∏–≥–º–∞',
    r'smartec', r'beward', r'dssl', r'fort', r'tantos',
    
    # –û–±—â–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
    r'–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç', r'–æ—Ö—Ä–∞–Ω–∞', r'security', r'—Å–∏—Å—Ç–µ–º–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç',
    r'–∫–æ–º–ø–ª–µ–∫—Å –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç', r'–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º',
    
    # –£–º–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏
    r'—É–º–Ω—ã–π –¥–æ–º', r'smart home', r'iot', r'–∏–Ω—Ç–µ—Ä–Ω–µ—Ç –≤–µ—â–µ–π',
    r'–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è', r'–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω',
    
    # –°–µ—Ç–∏
    r'ip-—Å–∏—Å—Ç–µ–º', r'—Å–µ—Ç–µ–≤', r'ethernet', r'poe', r'wi-fi –∫–∞–º–µ—Ä',
    r'–æ–±–ª–∞—á–Ω', r'cloud'
]

def check_keywords(text: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ"""
    if not text:
        return False
    
    text_lower = text.lower()
    
    for keyword in KEYWORDS:
        if re.search(keyword, text_lower):
            return True
    
    return False


# ============ –°–û–•–†–ê–ù–ï–ù–ò–ï –í –ë–ê–ó–£ ============
async def save_news(source: str, title: str, content: str, image_url: str = None, post_url: str = None):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–æ–≤–æ—Å—Ç—å –≤ Supabase —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
    combined_text = f"{title} {content}"
    if not check_keywords(combined_text):
        logger.info(f"‚è≠Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω–æ (–Ω–µ—Ç –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤): {title[:50]}...")
        return None
    
    # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê: –ù–ï —Å–æ—Ö—Ä–∞–Ω—è–µ–º placeholder-—ã!
    if image_url:
        placeholder_patterns = [
            'R0lGODlhAQABAIABAP',          # 1x1 –ø—Ä–æ–∑—Ä–∞—á–Ω—ã–π GIF
            'R0lGODlhAQABAIAAAA',          # 1x1 –ª—é–±–æ–π —Ü–≤–µ—Ç GIF  
            'PHN2ZyB4bWxu',                 # SVG placeholder
            'PHN2ZyB4bWxuc',                # SVG –≤–∞—Ä–∏–∞—Ü–∏–∏
            'data:image/svg+xml;base64,PHN2', # SVG base64
            'data:image/gif;base64,R0lGOD', # –ú–∞–ª–µ–Ω—å–∫–∏–µ GIF
            'placeholder',
            'blank.gif',
            'blank.png',
            'loading.gif',
            '1x1.gif',
            '1x1.png',
        ]
        
        # –ï—Å–ª–∏ —ç—Ç–æ placeholder - —Å–æ—Ö—Ä–∞–Ω—è–µ–º NULL –≤–º–µ—Å—Ç–æ –Ω–µ–≥–æ
        for pattern in placeholder_patterns:
            if pattern in image_url:
                logger.info(f"‚ö†Ô∏è  –û–±–Ω–∞—Ä—É–∂–µ–Ω placeholder, —Å–æ—Ö—Ä–∞–Ω—è–µ–º NULL")
                image_url = None
                break
    
    try:
        data = {
            "source": source,
            "title": title,
            "content": content,
            "image_url": image_url,
            "post_url": post_url,
            "deleted": False
        }
        result = supabase.table("news").insert(data).execute()
        logger.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {title[:50]}...")
        if image_url:
            logger.info(f"üì∏ –° –∫–∞—Ä—Ç–∏–Ω–∫–æ–π: {image_url[:60]}...")
        return result
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")
        return None


# ============ –ö–û–ú–ê–ù–î–´ –ë–û–¢–ê ============
async def start_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–ö–æ–º–∞–Ω–¥–∞ /start"""
    await update.message.reply_text(
        "ü§ñ –ë–æ—Ç –Ω–æ–≤–æ—Å—Ç–µ–π IQ Safety –∑–∞–ø—É—â–µ–Ω!\n\n"
        "üìã –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã:\n"
        "/parse - –ó–∞–ø—É—Å—Ç–∏—Ç—å –ø–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–µ–π –≤—Ä—É—á–Ω—É—é\n"
        "/list - –ü–æ–∫–∞–∑–∞—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ —Å ID\n"
        "/delete <ID> - –£–¥–∞–ª–∏—Ç—å –Ω–æ–≤–æ—Å—Ç—å –ø–æ ID\n"
        "/stats - –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π\n"
        "/sources - –°–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"
    )


async def parse_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–ö–æ–º–∞–Ω–¥–∞ /parse - –∑–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤—Ä—É—á–Ω—É—é"""
    await update.message.reply_text("üîÑ –ó–∞–ø—É—Å–∫–∞—é –ø–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–µ–π...")
    
    try:
        await parse_all_sites()
        await update.message.reply_text("‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à—ë–Ω! –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö.")
    except Exception as e:
        await update.message.reply_text(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
        logger.error(f"–û—à–∏–±–∫–∞ –≤ parse_command: {e}")


async def list_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–ö–æ–º–∞–Ω–¥–∞ /list - –ø–æ–∫–∞–∑–∞—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏"""
    try:
        response = supabase.table('news').select('id, title, source, deleted').order('created_at', desc=True).limit(15).execute()
        
        if not response.data:
            await update.message.reply_text("üì≠ –ù–æ–≤–æ—Å—Ç–µ–π –ø–æ–∫–∞ –Ω–µ—Ç")
            return
        
        message = "üì∞ –ü–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏:\n\n"
        for news in response.data:
            deleted_mark = " ‚ùå" if news.get('deleted') else ""
            source = news.get('source', 'N/A')
            title = news.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')[:60]
            message += f"ID: {news['id']} | {source}\n{title}...{deleted_mark}\n\n"
        
        await update.message.reply_text(message)
    except Exception as e:
        await update.message.reply_text(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        logger.error(f"–û—à–∏–±–∫–∞ –≤ list_command: {e}")


async def delete_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–ö–æ–º–∞–Ω–¥–∞ /delete <ID> - –ø–æ–º–µ—Ç–∏—Ç—å –Ω–æ–≤–æ—Å—Ç—å –∫–∞–∫ —É–¥–∞–ª—ë–Ω–Ω—É—é"""
    if not context.args:
        await update.message.reply_text("‚ùå –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: /delete <ID –Ω–æ–≤–æ—Å—Ç–∏>\n\n–ü—Ä–∏–º–µ—Ä: /delete 5")
        return
    
    try:
        news_id = int(context.args[0])
        
        response = supabase.table('news').update({
            'deleted': True
        }).eq('id', news_id).execute()
        
        if response.data:
            await update.message.reply_text(f"‚úÖ –ù–æ–≤–æ—Å—Ç—å #{news_id} —É–¥–∞–ª–µ–Ω–∞ –∏–∑ –≤–∏–¥–∂–µ—Ç–∞")
        else:
            await update.message.reply_text(f"‚ùå –ù–æ–≤–æ—Å—Ç—å #{news_id} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
    except ValueError:
        await update.message.reply_text("‚ùå ID –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —á–∏—Å–ª–æ–º!")
    except Exception as e:
        await update.message.reply_text(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        logger.error(f"–û—à–∏–±–∫–∞ –≤ delete_command: {e}")


async def stats_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–ö–æ–º–∞–Ω–¥–∞ /stats - —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π"""
    try:
        all_news = supabase.table('news').select('id, source, deleted').execute()
        
        total = len(all_news.data)
        deleted = len([n for n in all_news.data if n.get('deleted')])
        active = total - deleted
        
        sources = {}
        for news in all_news.data:
            if not news.get('deleted'):
                source = news.get('source', 'Unknown')
                sources[source] = sources.get(source, 0) + 1
        
        message = f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π:\n\n"
        message += f"üì∞ –í—Å–µ–≥–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {total}\n"
        message += f"‚úÖ –ê–∫—Ç–∏–≤–Ω—ã—Ö: {active}\n"
        message += f"‚ùå –£–¥–∞–ª—ë–Ω–Ω—ã—Ö: {deleted}\n\n"
        message += "üìç –ê–∫—Ç–∏–≤–Ω—ã–µ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º:\n"
        for source, count in sorted(sources.items(), key=lambda x: x[1], reverse=True):
            message += f"  ‚Ä¢ {source}: {count}\n"
        
        await update.message.reply_text(message)
    except Exception as e:
        await update.message.reply_text(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        logger.error(f"–û—à–∏–±–∫–∞ –≤ stats_command: {e}")


async def sources_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–ö–æ–º–∞–Ω–¥–∞ /sources - —Å–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
    message = """üåê –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π:

üåç –ó–∞—Ä—É–±–µ–∂–Ω—ã–µ:
‚Ä¢ Hikvision
‚Ä¢ Dahua
‚Ä¢ Axis
‚Ä¢ Bosch
‚Ä¢ Siemens
‚Ä¢ Hochiki
‚Ä¢ Hanwha Vision
‚Ä¢ Cloudflare

üá∑üá∫ –†–æ—Å—Å–∏–π—Å–∫–∏–µ:
‚Ä¢ –ë–æ–ª–∏–¥
‚Ä¢ –†—É–±–µ–∂
‚Ä¢ Perco
‚Ä¢ DSSL
‚Ä¢ RGSec
‚Ä¢ SKUD-System

üá∞üáø –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω—Å–∫–∏–µ:
‚Ä¢ Orion M2M
‚Ä¢ Intant
‚Ä¢ Inform.kz

üìã –í—Å–µ–≥–æ: 21 –∏—Å—Ç–æ—á–Ω–∏–∫
üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ: –∫–∞–∂–¥—ã–µ 2 —á–∞—Å–∞
üîç –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è: ~90 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
    
    await update.message.reply_text(message)


# ============ TELEGRAM HANDLER ============
async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ –≤—Ö–æ–¥—è—â–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è"""
    message = update.message or update.channel_post
    if not message:
        return
    
    text = message.text or message.caption or ""
    
    if text.startswith('/'):
        return
    
    image_url = None
    if message.photo:
        photo = message.photo[-1]
        file = await context.bot.get_file(photo.file_id)
        image_url = file.file_path
    
    post_url = None
    if update.channel_post:
        post_url = f"https://t.me/{CHANNEL_ID.replace('@', '')}/{message.message_id}"
    
    title = text.split('\n')[0][:100] if text else "–ù–æ–≤–æ—Å—Ç—å IQ Safety"
    
    # –î–õ–Ø –ö–ê–ù–ê–õ–ê IQ SAFETY - –ë–ï–ó –§–ò–õ–¨–¢–†–ê!
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å—ë —á—Ç–æ –ø—É–±–ª–∏–∫—É–µ—Ç—Å—è –≤ –∫–∞–Ω–∞–ª–µ
    if update.channel_post:
        try:
            data = {
                "source": "IQ Safety",
                "title": title,
                "content": text,
                "image_url": image_url,
                "post_url": post_url,
                "deleted": False
            }
            result = supabase.table("news").insert(data).execute()
            logger.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –∏–∑ –∫–∞–Ω–∞–ª–∞: {title[:50]}...")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")
    else:
        # –î–ª—è –ª–∏—á–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –±–æ—Ç—É - —Å —Ñ–∏–ª—å—Ç—Ä–æ–º
        result = await save_news(
            source="IQ Safety",
            title=title,
            content=text,
            image_url=image_url,
            post_url=post_url
        )
        
        if update.message and result:
            try:
                if image_url:
                    await context.bot.send_photo(
                        chat_id=CHANNEL_ID,
                        photo=image_url,
                        caption=text
                    )
                else:
                    await context.bot.send_message(
                        chat_id=CHANNEL_ID,
                        text=text
                    )
                await update.message.reply_text("‚úÖ –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ –≤ –∫–∞–Ω–∞–ª–µ!")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –≤ –∫–∞–Ω–∞–ª: {e}")
                await update.message.reply_text(f"‚ùå –û—à–∏–±–∫–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {e}")


# ============ –£–ù–ò–í–ï–†–°–ê–õ–¨–ù–´–ô –ü–ê–†–°–ï–† –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ô ============
def extract_image(item, base_url: str) -> Optional[str]:
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å–æ –≤—Å–µ–º–∏ —É—Å–ª–æ–≤–∏—è–º–∏"""
    
    # –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –≤–æ–∑–º–æ–∂–Ω—ã—Ö —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è –∫–∞—Ä—Ç–∏–Ω–æ–∫
    img_selectors = [
        'img',
        'picture img',
        'picture source',
        '.image img',
        '.thumbnail img',
        '.news-image img',
        '.post-image img',
        '.featured-image img',
        '[class*="image"] img',
        '[class*="photo"] img',
        '[class*="pic"] img',
    ]
    
    # –ò—â–µ–º –ø–æ –≤—Å–µ–º —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º
    img_elements = []
    for selector in img_selectors:
        elements = item.select(selector)
        img_elements.extend(elements)
    
    if not img_elements:
        return None
    
    # –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –≤–æ–∑–º–æ–∂–Ω—ã—Ö –∞—Ç—Ä–∏–±—É—Ç–æ–≤ –≥–¥–µ –º–æ–∂–µ—Ç –±—ã—Ç—å URL –∫–∞—Ä—Ç–∏–Ω–∫–∏
    # –í–ê–ñ–ù–û: –ø–æ—Ä—è–¥–æ–∫ –∏–º–µ–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ! –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º data-–∞—Ç—Ä–∏–±—É—Ç—ã (—Ä–µ–∞–ª—å–Ω—ã–µ –∫–∞—Ä—Ç–∏–Ω–∫–∏)
    img_attributes = [
        'data-original',      # –ß–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è lazy loading
        'data-src',           # –ü–æ–ø—É–ª—è—Ä–Ω—ã–π –¥–ª—è lazy loading
        'data-lazy-src',      # Lazy load
        'data-srcset',        # Responsive lazy loading
        'data-image',         # –ö–∞—Å—Ç–æ–º–Ω—ã–π –∞—Ç—Ä–∏–±—É—Ç
        'data-url',           # –ö–∞—Å—Ç–æ–º–Ω—ã–π –∞—Ç—Ä–∏–±—É—Ç
        'srcset',             # Responsive images
        'src',                # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π (–º–æ–∂–µ—Ç –±—ã—Ç—å placeholder!)
    ]
    
    best_image = None
    best_score = 0
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—ã–π –Ω–∞–π–¥–µ–Ω–Ω—ã–π img —ç–ª–µ–º–µ–Ω—Ç
    for img_el in img_elements:
        # –ü—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å URL –∏–∑ –∞—Ç—Ä–∏–±—É—Ç–æ–≤
        for attr in img_attributes:
            value = img_el.get(attr)
            if not value:
                continue
            
            # –ï—Å–ª–∏ srcset - –±–µ—Ä—ë–º –ø–µ—Ä–≤—É—é (–æ–±—ã—á–Ω–æ —Å–∞–º—É—é –±–æ–ª—å—à—É—é) –∫–∞—Ä—Ç–∏–Ω–∫—É
            if 'srcset' in attr and ' ' in value:
                value = value.split(',')[0].split(' ')[0].strip()
            
            value = value.strip()
            
            if not value or len(value) < 10:
                continue
            
            # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê: –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∏–∑–≤–µ—Å—Ç–Ω—ã–µ placeholder-—ã
            placeholder_signatures = [
                'R0lGODlhAQABAIABAP',          # 1x1 transparent GIF
                'R0lGODlhAQABAIAAAA',          # 1x1 –ª—é–±–æ–π —Ü–≤–µ—Ç GIF
                'PHN2ZyB4bWxu',                 # SVG placeholder
                'data:image/gif;base64,R0lGOD', # –ö–æ—Ä–æ—Ç–∫–∏–µ GIF
                'data:image/svg+xml',           # SVG –≤ base64
                '//:0',                         # –ü—É—Å—Ç–æ–π –ø—Ä–æ—Ç–æ–∫–æ–ª
                'placeholder',
                'blank',
                'loading',
                'spinner',
                'default',
                'noimage',
                'no-image',
            ]
            
            is_placeholder = False
            for sig in placeholder_signatures:
                if sig in value:
                    is_placeholder = True
                    break
            
            if is_placeholder:
                continue
            
            # –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ URL
            score = 0
            
            # –ë–æ–Ω—É—Å—ã –∑–∞ data-–∞—Ç—Ä–∏–±—É—Ç—ã (–æ–±—ã—á–Ω–æ —Ç–∞–º —Ä–µ–∞–ª—å–Ω—ã–µ –∫–∞—Ä—Ç–∏–Ω–∫–∏)
            if attr.startswith('data-'):
                score += 10
            
            # –ë–æ–Ω—É—Å—ã –∑–∞ –¥–ª–∏–Ω—É URL (–¥–ª–∏–Ω–Ω—ã–µ –æ–±—ã—á–Ω–æ —Ä–µ–∞–ª—å–Ω—ã–µ)
            if len(value) > 50:
                score += 5
            
            # –ë–æ–Ω—É—Å—ã –∑–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            if any(ext in value.lower() for ext in ['.jpg', '.jpeg', '.png', '.webp']):
                score += 3
            
            # –®—Ç—Ä–∞—Ñ—ã –∑–∞ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            if any(bad in value.lower() for bad in ['icon', 'logo', 'avatar', 'thumb']):
                score -= 5
            
            # –ï—Å–ª–∏ —ç—Ç–æ –ª—É—á—à–∏–π –Ω–∞–π–¥–µ–Ω–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç - —Å–æ—Ö—Ä–∞–Ω—è–µ–º
            if score > best_score:
                best_score = score
                best_image = value
    
    if not best_image:
        return None
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –ø—É—Ç–∏ –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ
    if best_image.startswith('//'):
        best_image = f"https:{best_image}"
    elif best_image.startswith('/'):
        best_image = f"{base_url}{best_image}"
    elif not best_image.startswith('http') and not best_image.startswith('data:'):
        best_image = f"{base_url}/{best_image}"
    
    return best_image


# ============ –£–ù–ò–í–ï–†–°–ê–õ–¨–ù–´–ô –ü–ê–†–°–ï–† –ù–û–í–û–°–¢–ï–ô ============
async def parse_generic_site(site_name: str, url: str, selectors: dict) -> int:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è –ª—é–±–æ–≥–æ —Å–∞–π—Ç–∞
    
    selectors = {
        'items': '.news-item, article',  # CSS —Å–µ–ª–µ–∫—Ç–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π
        'title': 'h2, h3, .title',       # CSS —Å–µ–ª–µ–∫—Ç–æ—Ä –∑–∞–≥–æ–ª–æ–≤–∫–∞
        'link': 'a[href]',               # CSS —Å–µ–ª–µ–∫—Ç–æ—Ä —Å—Å—ã–ª–∫–∏
        'content': 'p, .description',    # CSS —Å–µ–ª–µ–∫—Ç–æ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    }
    """
    logger.info(f"üåê –ü–∞—Ä—Å–∏–Ω–≥ {site_name}...")
    html = await fetch_html(url)
    if not html:
        return 0
    
    soup = BeautifulSoup(html, 'html.parser')
    news_items = soup.select(selectors.get('items', 'article'))[:5]
    
    count = 0
    for item in news_items:
        try:
            # –ó–∞–≥–æ–ª–æ–≤–æ–∫
            title_el = item.select_one(selectors.get('title', 'h2, h3'))
            title = title_el.get_text(strip=True) if title_el else None
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
            if not title or len(title) < 15:
                continue
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–µ–ª–µ—Ñ–æ–Ω—ã
            if title.startswith('+'):
                continue
            
            # –°—Å—ã–ª–∫–∞ - —Å–Ω–∞—á–∞–ª–∞ –∏—â–µ–º –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ, –ø–æ—Ç–æ–º –≤–µ–∑–¥–µ
            link = None
            
            # 1. –°—Å—ã–ª–∫–∞ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç!)
            if title_el and title_el.name == 'a':
                link = title_el.get('href')
            elif title_el:
                title_link = title_el.find_parent('a') or title_el.find('a')
                if title_link:
                    link = title_link.get('href')
            
            # 2. –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ - –∏—â–µ–º –ø–µ—Ä–≤—É—é —Å—Å—ã–ª–∫—É –≤ —ç–ª–µ–º–µ–Ω—Ç–µ
            if not link:
                link_el = item.select_one(selectors.get('link', 'a[href]'))
                link = link_el.get('href') if link_el else None
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è —Å—Å—ã–ª–∫–∏
            if link:
                if link.startswith('tel:') or link.startswith('mailto:'):
                    link = None
                elif not link.startswith('http'):
                    base = url.rsplit('/', 1)[0] if '/' in url else url
                    link = f"{base}{link}" if link.startswith('/') else f"{base}/{link}"
            
            # –ö–æ–Ω—Ç–µ–Ω—Ç
            content_el = item.select_one(selectors.get('content', 'p, .description'))
            content = content_el.get_text(strip=True) if content_el else title
            
            # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (—É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä)
            base_url = '/'.join(url.split('/')[:3])  # https://example.com
            image = extract_image(item, base_url)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            if title and len(title) > 10:
                existing = supabase.table("news").select("id").eq("title", title).execute()
                if not existing.data:
                    result = await save_news(site_name, title, content or "", image, link)
                    if result:
                        count += 1
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {site_name}: {e}")
    
    logger.info(f"üìä {site_name}: –¥–æ–±–∞–≤–ª–µ–Ω–æ {count} –Ω–æ–≤–æ—Å—Ç–µ–π")
    return count


# ============ –ü–ê–†–°–ò–ù–ì –°–ê–ô–¢–û–í ============
async def fetch_html(url: str) -> Optional[str]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        async with aiohttp.ClientSession() as session:
            async with session.get(url, timeout=30, headers=headers) as response:
                if response.status == 200:
                    return await response.text()
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}: {e}")
    return None


# ============ –ó–ê–†–£–ë–ï–ñ–ù–´–ï –°–ê–ô–¢–´ ============

async def parse_hikvision():
    """Hikvision - —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä"""
    logger.info("üåê –ü–∞—Ä—Å–∏–Ω–≥ Hikvision...")
    url = "https://www.hikvision.com/en/newsroom/latest-news/"
    html = await fetch_html(url)
    if not html:
        return 0
    
    soup = BeautifulSoup(html, 'html.parser')
    
    # Hikvision –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–∞–∑–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã - –ø—Ä–æ–±—É–µ–º –≤—Å–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
    news_items = (
        soup.select('article') or 
        soup.select('.news-item') or 
        soup.select('.content-item') or
        soup.select('[class*="card"]') or
        soup.select('.latest-news-item')
    )[:5]
    
    count = 0
    for item in news_items:
        try:
            # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            title_el = (
                item.select_one('h2 a') or 
                item.select_one('h3 a') or 
                item.select_one('.title a') or
                item.select_one('a h2') or
                item.select_one('a h3') or
                item.select_one('h2') or
                item.select_one('h3')
            )
            
            if not title_el:
                continue
            
            title = title_el.get_text(strip=True)
            
            if not title or len(title) < 15:
                continue
            
            # –ò—â–µ–º —Å—Å—ã–ª–∫—É - –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤
            link = None
            
            # –í–∞—Ä–∏–∞–Ω—Ç 1: –°—Å—ã–ª–∫–∞ –≤ —Å–∞–º–æ–º –∑–∞–≥–æ–ª–æ–≤–∫–µ
            if title_el.name == 'a':
                link = title_el.get('href')
            
            # –í–∞—Ä–∏–∞–Ω—Ç 2: –†–æ–¥–∏—Ç–µ–ª—å –∑–∞–≥–æ–ª–æ–≤–∫–∞ - —Å—Å—ã–ª–∫–∞
            if not link and title_el.parent and title_el.parent.name == 'a':
                link = title_el.parent.get('href')
            
            # –í–∞—Ä–∏–∞–Ω—Ç 3: –°—Å—ã–ª–∫–∞ –≤–Ω—É—Ç—Ä–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞
            if not link:
                inner_link = title_el.find('a')
                if inner_link:
                    link = inner_link.get('href')
            
            # –í–∞—Ä–∏–∞–Ω—Ç 4: –ü–µ—Ä–≤–∞—è —Å—Å—ã–ª–∫–∞ –≤ —ç–ª–µ–º–µ–Ω—Ç–µ –Ω–æ–≤–æ—Å—Ç–∏
            if not link:
                first_link = item.select_one('a[href]')
                if first_link:
                    link = first_link.get('href')
            
            # –í–∞—Ä–∏–∞–Ω—Ç 5: –°—Å—ã–ª–∫–∞ –≤ –∞—Ç—Ä–∏–±—É—Ç–µ data-url –∏–ª–∏ data-link
            if not link:
                link = item.get('data-url') or item.get('data-link')
            
            # –î–µ–ª–∞–µ–º –∞–±—Å–æ–ª—é—Ç–Ω—É—é —Å—Å—ã–ª–∫—É
            if link:
                if link.startswith('/'):
                    link = f"https://www.hikvision.com{link}"
                elif not link.startswith('http'):
                    link = f"https://www.hikvision.com/en/newsroom/{link}"
            else:
                # –ï—Å–ª–∏ —Å—Å—ã–ª–∫—É –Ω–µ –Ω–∞—à–ª–∏ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –Ω–æ–≤–æ—Å—Ç–µ–π
                link = "https://www.hikvision.com/en/newsroom/latest-news/"
            
            # –ö–æ–Ω—Ç–µ–Ω—Ç
            content_el = (
                item.select_one('p') or 
                item.select_one('.description') or
                item.select_one('.excerpt')
            )
            content = content_el.get_text(strip=True) if content_el else title
            
            # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            image = extract_image(item, 'https://www.hikvision.com')
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º
            if title:
                existing = supabase.table("news").select("id").eq("title", title).execute()
                if not existing.data:
                    result = await save_news("Hikvision", title, content, image, link)
                    if result:
                        count += 1
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Hikvision: {e}")
    
    logger.info(f"üìä Hikvision: –¥–æ–±–∞–≤–ª–µ–Ω–æ {count} –Ω–æ–≤–æ—Å—Ç–µ–π")
    return count


async def parse_bolid():
    """–ë–æ–ª–∏–¥"""
    logger.info("üá∑üá∫ –ü–∞—Ä—Å–∏–Ω–≥ –ë–æ–ª–∏–¥...")
    url = "https://bolid.ru/about/news/"
    html = await fetch_html(url)
    if not html:
        return 0
    
    soup = BeautifulSoup(html, 'html.parser')
    news_items = soup.select('.news-item, article, .news-list-item')[:5]
    
    count = 0
    for item in news_items:
        try:
            title_el = item.select_one('h2, h3, .title, a')
            title = title_el.get_text(strip=True) if title_el else None
            
            link_el = item.select_one('a[href]')
            link = link_el['href'] if link_el else None
            if link and not link.startswith('http'):
                link = f"https://bolid.ru{link}"
            
            content_el = item.select_one('p, .description, .anons')
            content = content_el.get_text(strip=True) if content_el else title
            
            img_el = item.select_one('img')
            image = img_el.get('src') if img_el else None
            if image and not image.startswith('http'):
                image = f"https://bolid.ru{image}"
            
            if title and len(title) > 10:
                existing = supabase.table("news").select("id").eq("title", title).execute()
                if not existing.data:
                    result = await save_news("–ë–æ–ª–∏–¥", title, content or "", image, link)
                    if result:
                        count += 1
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –ë–æ–ª–∏–¥: {e}")
    
    logger.info(f"üìä –ë–æ–ª–∏–¥: –¥–æ–±–∞–≤–ª–µ–Ω–æ {count} –Ω–æ–≤–æ—Å—Ç–µ–π")
    return count


async def parse_perco():
    """Perco"""
    logger.info("üåê –ü–∞—Ä—Å–∏–Ω–≥ Perco...")
    url = "https://www.perco.ru/novosti/"
    html = await fetch_html(url)
    if not html:
        return 0
    
    soup = BeautifulSoup(html, 'html.parser')
    news_items = soup.select('.news-item, article, .post')[:5]
    
    count = 0
    for item in news_items:
        try:
            title_el = item.select_one('h2, h3, .title, a')
            title = title_el.get_text(strip=True) if title_el else None
            
            link_el = item.select_one('a[href]')
            link = link_el['href'] if link_el else None
            if link and not link.startswith('http'):
                link = f"https://www.perco.ru{link}"
            
            content_el = item.select_one('p, .description')
            content = content_el.get_text(strip=True) if content_el else title
            
            img_el = item.select_one('img')
            image = img_el.get('src') if img_el else None
            
            if title and len(title) > 10:
                existing = supabase.table("news").select("id").eq("title", title).execute()
                if not existing.data:
                    result = await save_news("Perco", title, content or "", image, link)
                    if result:
                        count += 1
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Perco: {e}")
    
    logger.info(f"üìä Perco: –¥–æ–±–∞–≤–ª–µ–Ω–æ {count} –Ω–æ–≤–æ—Å—Ç–µ–π")
    return count


async def parse_dahua():
    """Dahua"""
    logger.info("üåê –ü–∞—Ä—Å–∏–Ω–≥ Dahua...")
    url = "https://www.dahuasecurity.com/ea/newsEvents"
    html = await fetch_html(url)
    if not html:
        return 0
    
    soup = BeautifulSoup(html, 'html.parser')
    news_items = soup.select('.news-item, article, .news-list li')[:5]
    
    count = 0
    for item in news_items:
        try:
            title_el = item.select_one('h2, h3, .title, a')
            title = title_el.get_text(strip=True) if title_el else None
            
            link_el = item.select_one('a[href]')
            link = link_el['href'] if link_el else None
            if link and not link.startswith('http'):
                link = f"https://www.dahuasecurity.com{link}"
            
            content_el = item.select_one('p, .description')
            content = content_el.get_text(strip=True) if content_el else title
            
            img_el = item.select_one('img')
            image = img_el.get('src') if img_el else None
            
            if title and len(title) > 10:
                existing = supabase.table("news").select("id").eq("title", title).execute()
                if not existing.data:
                    result = await save_news("Dahua", title, content or "", image, link)
                    if result:
                        count += 1
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Dahua: {e}")
    
    logger.info(f"üìä Dahua: –¥–æ–±–∞–≤–ª–µ–Ω–æ {count} –Ω–æ–≤–æ—Å—Ç–µ–π")
    return count


async def parse_axis():
    """Axis Communications"""
    logger.info("üåê –ü–∞—Ä—Å–∏–Ω–≥ Axis...")
    url = "https://newsroom.axis.com/"
    html = await fetch_html(url)
    if not html:
        return 0
    
    soup = BeautifulSoup(html, 'html.parser')
    news_items = soup.select('article, .news-item, .post')[:5]
    
    count = 0
    for item in news_items:
        try:
            title_el = item.select_one('h2, h3, .title, a')
            title = title_el.get_text(strip=True) if title_el else None
            
            link_el = item.select_one('a[href]')
            link = link_el['href'] if link_el else None
            if link and not link.startswith('http'):
                link = f"https://newsroom.axis.com{link}"
            
            content_el = item.select_one('p, .description')
            content = content_el.get_text(strip=True) if content_el else title
            
            # –ò—â–µ–º –∫–∞—Ä—Ç–∏–Ω–∫—É –≤ picture > source –∏–ª–∏ img
            image = None
            picture_el = item.select_one('picture source[srcset]')
            if picture_el:
                srcset = picture_el.get('srcset', '')
                # –ë–µ—Ä—ë–º –ø–µ—Ä–≤—ã–π URL –∏–∑ srcset
                if srcset:
                    image = srcset.split(',')[0].split(' ')[0].strip()
                    if image and not image.startswith('http'):
                        image = f"https://newsroom.axis.com{image}"
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –≤ picture, –∏—â–µ–º –æ–±—ã—á–Ω—ã–π img
            if not image:
                img_el = item.select_one('img')
                if img_el:
                    image = img_el.get('src') or img_el.get('data-src')
                    if image and not image.startswith('http'):
                        image = f"https://newsroom.axis.com{image}"
            
            if title and len(title) > 10:
                existing = supabase.table("news").select("id").eq("title", title).execute()
                if not existing.data:
                    result = await save_news("Axis", title, content or "", image, link)
                    if result:
                        count += 1
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Axis: {e}")
    
    logger.info(f"üìä Axis: –¥–æ–±–∞–≤–ª–µ–Ω–æ {count} –Ω–æ–≤–æ—Å—Ç–µ–π")
    return count


async def parse_bosch():
    """Bosch"""
    logger.info("üåê –ü–∞—Ä—Å–∏–Ω–≥ Bosch...")
    url = "https://www.boschbuildingtechnologies.com/lifesafetysystems/en/news-events/"
    html = await fetch_html(url)
    if not html:
        return 0
    
    soup = BeautifulSoup(html, 'html.parser')
    news_items = soup.select('article, .news-item, .content-item')[:5]
    
    count = 0
    for item in news_items:
        try:
            title_el = item.select_one('h2, h3, .title, a')
            title = title_el.get_text(strip=True) if title_el else None
            
            link_el = item.select_one('a[href]')
            link = link_el['href'] if link_el else None
            if link and not link.startswith('http'):
                link = f"https://www.boschbuildingtechnologies.com{link}"
            
            content_el = item.select_one('p, .description')
            content = content_el.get_text(strip=True) if content_el else title
            
            img_el = item.select_one('img')
            image = img_el.get('src') if img_el else None
            
            if title and len(title) > 10:
                existing = supabase.table("news").select("id").eq("title", title).execute()
                if not existing.data:
                    result = await save_news("Bosch", title, content or "", image, link)
                    if result:
                        count += 1
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Bosch: {e}")
    
    logger.info(f"üìä Bosch: –¥–æ–±–∞–≤–ª–µ–Ω–æ {count} –Ω–æ–≤–æ—Å—Ç–µ–π")
    return count


async def parse_siemens():
    """Siemens"""
    logger.info("üåê –ü–∞—Ä—Å–∏–Ω–≥ Siemens...")
    url = "https://press.siemens.com/global/en"
    html = await fetch_html(url)
    if not html:
        return 0
    
    soup = BeautifulSoup(html, 'html.parser')
    news_items = soup.select('article, .news-item, .press-release')[:5]
    
    count = 0
    for item in news_items:
        try:
            title_el = item.select_one('h2, h3, .title, a')
            title = title_el.get_text(strip=True) if title_el else None
            
            link_el = item.select_one('a[href]')
            link = link_el['href'] if link_el else None
            if link and not link.startswith('http'):
                link = f"https://press.siemens.com{link}"
            
            content_el = item.select_one('p, .description, .excerpt')
            content = content_el.get_text(strip=True) if content_el else title
            
            img_el = item.select_one('img')
            image = img_el.get('src') if img_el else None
            
            if title and len(title) > 10:
                existing = supabase.table("news").select("id").eq("title", title).execute()
                if not existing.data:
                    result = await save_news("Siemens", title, content or "", image, link)
                    if result:
                        count += 1
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Siemens: {e}")
    
    logger.info(f"üìä Siemens: –¥–æ–±–∞–≤–ª–µ–Ω–æ {count} –Ω–æ–≤–æ—Å—Ç–µ–π")
    return count


async def parse_hochiki():
    """Hochiki"""
    logger.info("üåê –ü–∞—Ä—Å–∏–Ω–≥ Hochiki...")
    url = "https://www.hochikieurope.com/news"
    html = await fetch_html(url)
    if not html:
        return 0
    
    soup = BeautifulSoup(html, 'html.parser')
    news_items = soup.select('article, .news-item, .post')[:5]
    
    count = 0
    for item in news_items:
        try:
            title_el = item.select_one('h2, h3, .title, a')
            title = title_el.get_text(strip=True) if title_el else None
            
            link_el = item.select_one('a[href]')
            link = link_el['href'] if link_el else None
            if link and not link.startswith('http'):
                link = f"https://www.hochikieurope.com{link}"
            
            content_el = item.select_one('p, .description')
            content = content_el.get_text(strip=True) if content_el else title
            
            img_el = item.select_one('img')
            image = img_el.get('src') if img_el else None
            
            if title and len(title) > 10:
                existing = supabase.table("news").select("id").eq("title", title).execute()
                if not existing.data:
                    result = await save_news("Hochiki", title, content or "", image, link)
                    if result:
                        count += 1
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Hochiki: {e}")
    
    logger.info(f"üìä Hochiki: –¥–æ–±–∞–≤–ª–µ–Ω–æ {count} –Ω–æ–≤–æ—Å—Ç–µ–π")
    return count


async def parse_rubezh():
    """–†—É–±–µ–∂"""
    logger.info("üá∑üá∫ –ü–∞—Ä—Å–∏–Ω–≥ –†—É–±–µ–∂...")
    url = "https://rubezh.ru/news"
    html = await fetch_html(url)
    if not html:
        return 0
    
    soup = BeautifulSoup(html, 'html.parser')
    news_items = soup.select('.news-item, article, .post')[:5]
    
    count = 0
    for item in news_items:
        try:
            title_el = item.select_one('h2, h3, .title, a')
            title = title_el.get_text(strip=True) if title_el else None
            
            link_el = item.select_one('a[href]')
            link = link_el['href'] if link_el else None
            if link and not link.startswith('http'):
                link = f"https://rubezh.ru{link}"
            
            content_el = item.select_one('p, .description')
            content = content_el.get_text(strip=True) if content_el else title
            
            img_el = item.select_one('img')
            image = img_el.get('src') if img_el else None
            
            if title and len(title) > 10:
                existing = supabase.table("news").select("id").eq("title", title).execute()
                if not existing.data:
                    result = await save_news("–†—É–±–µ–∂", title, content or "", image, link)
                    if result:
                        count += 1
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –†—É–±–µ–∂: {e}")
    
    logger.info(f"üìä –†—É–±–µ–∂: –¥–æ–±–∞–≤–ª–µ–Ω–æ {count} –Ω–æ–≤–æ—Å—Ç–µ–π")
    return count


async def parse_hanwha():
    """Hanwha Vision"""
    logger.info("üåê –ü–∞—Ä—Å–∏–Ω–≥ Hanwha...")
    url = "https://www.hanwhavision.com/en/news-center/news-hub/"
    html = await fetch_html(url)
    if not html:
        return 0
    
    soup = BeautifulSoup(html, 'html.parser')
    news_items = soup.select('article, .news-item, .post')[:5]
    
    count = 0
    for item in news_items:
        try:
            title_el = item.select_one('h2, h3, .title, a')
            title = title_el.get_text(strip=True) if title_el else None
            
            link_el = item.select_one('a[href]')
            link = link_el['href'] if link_el else None
            if link and not link.startswith('http'):
                link = f"https://www.hanwhavision.com{link}"
            
            content_el = item.select_one('p, .description')
            content = content_el.get_text(strip=True) if content_el else title
            
            img_el = item.select_one('img')
            image = img_el.get('src') if img_el else None
            
            if title and len(title) > 10:
                existing = supabase.table("news").select("id").eq("title", title).execute()
                if not existing.data:
                    result = await save_news("Hanwha", title, content or "", image, link)
                    if result:
                        count += 1
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Hanwha: {e}")
    
    logger.info(f"üìä Hanwha: –¥–æ–±–∞–≤–ª–µ–Ω–æ {count} –Ω–æ–≤–æ—Å—Ç–µ–π")
    return count


async def parse_cloudflare():
    """Cloudflare (–µ—Å–ª–∏ –µ—Å—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏)"""
    logger.info("üåê –ü–∞—Ä—Å–∏–Ω–≥ Cloudflare...")
    url = "https://cloudflare.net/news/default.aspx"
    html = await fetch_html(url)
    if not html:
        return 0
    
    soup = BeautifulSoup(html, 'html.parser')
    news_items = soup.select('article, .news-item, .post')[:5]
    
    count = 0
    for item in news_items:
        try:
            title_el = item.select_one('h2, h3, .title, a')
            title = title_el.get_text(strip=True) if title_el else None
            
            link_el = item.select_one('a[href]')
            link = link_el['href'] if link_el else None
            if link and not link.startswith('http'):
                link = f"https://cloudflare.net{link}"
            
            content_el = item.select_one('p, .description')
            content = content_el.get_text(strip=True) if content_el else title
            
            img_el = item.select_one('img')
            image = img_el.get('src') if img_el else None
            
            if title and len(title) > 10:
                existing = supabase.table("news").select("id").eq("title", title).execute()
                if not existing.data:
                    result = await save_news("Cloudflare", title, content or "", image, link)
                    if result:
                        count += 1
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Cloudflare: {e}")
    
    logger.info(f"üìä Cloudflare: –¥–æ–±–∞–≤–ª–µ–Ω–æ {count} –Ω–æ–≤–æ—Å—Ç–µ–π")
    return count


async def parse_rgsec():
    """RGSec"""
    logger.info("üá∑üá∫ –ü–∞—Ä—Å–∏–Ω–≥ RGSec...")
    url = "https://www.rgsec.ru/news"
    html = await fetch_html(url)
    if not html:
        return 0
    
    soup = BeautifulSoup(html, 'html.parser')
    news_items = soup.select('.news-item, article, .post')[:5]
    
    count = 0
    for item in news_items:
        try:
            title_el = item.select_one('h2, h3, .title, a')
            title = title_el.get_text(strip=True) if title_el else None
            
            link_el = item.select_one('a[href]')
            link = link_el['href'] if link_el else None
            if link and not link.startswith('http'):
                link = f"https://www.rgsec.ru{link}"
            
            content_el = item.select_one('p, .description')
            content = content_el.get_text(strip=True) if content_el else title
            
            img_el = item.select_one('img')
            image = img_el.get('src') if img_el else None
            
            if title and len(title) > 10:
                existing = supabase.table("news").select("id").eq("title", title).execute()
                if not existing.data:
                    result = await save_news("RGSec", title, content or "", image, link)
                    if result:
                        count += 1
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ RGSec: {e}")
    
    logger.info(f"üìä RGSec: –¥–æ–±–∞–≤–ª–µ–Ω–æ {count} –Ω–æ–≤–æ—Å—Ç–µ–π")
    return count


async def parse_dssl():
    """DSSL"""
    logger.info("üá∑üá∫ –ü–∞—Ä—Å–∏–Ω–≥ DSSL...")
    url = "https://www.dssl.ru/publications/news/"
    html = await fetch_html(url)
    if not html:
        return 0
    
    soup = BeautifulSoup(html, 'html.parser')
    news_items = soup.select('.news-item, article, .post')[:5]
    
    count = 0
    for item in news_items:
        try:
            title_el = item.select_one('h2, h3, .title, a')
            title = title_el.get_text(strip=True) if title_el else None
            
            link_el = item.select_one('a[href]')
            link = link_el['href'] if link_el else None
            if link and not link.startswith('http'):
                link = f"https://www.dssl.ru{link}"
            
            content_el = item.select_one('p, .description')
            content = content_el.get_text(strip=True) if content_el else title
            
            img_el = item.select_one('img')
            image = img_el.get('src') if img_el else None
            
            if title and len(title) > 10:
                existing = supabase.table("news").select("id").eq("title", title).execute()
                if not existing.data:
                    result = await save_news("DSSL", title, content or "", image, link)
                    if result:
                        count += 1
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ DSSL: {e}")
    
    logger.info(f"üìä DSSL: –¥–æ–±–∞–≤–ª–µ–Ω–æ {count} –Ω–æ–≤–æ—Å—Ç–µ–π")
    return count


async def parse_skud_system():
    """SKUD-System"""
    logger.info("üá∑üá∫ –ü–∞—Ä—Å–∏–Ω–≥ SKUD-System...")
    url = "https://skud-system.ru/news/"
    html = await fetch_html(url)
    if not html:
        return 0
    
    soup = BeautifulSoup(html, 'html.parser')
    news_items = soup.select('.news-item, article, .post')[:5]
    
    count = 0
    for item in news_items:
        try:
            title_el = item.select_one('h2, h3, .title, a')
            title = title_el.get_text(strip=True) if title_el else None
            
            link_el = item.select_one('a[href]')
            link = link_el['href'] if link_el else None
            if link and not link.startswith('http'):
                link = f"https://skud-system.ru{link}"
            
            content_el = item.select_one('p, .description')
            content = content_el.get_text(strip=True) if content_el else title
            
            img_el = item.select_one('img')
            image = img_el.get('src') if img_el else None
            
            if title and len(title) > 10:
                existing = supabase.table("news").select("id").eq("title", title).execute()
                if not existing.data:
                    result = await save_news("SKUD-System", title, content or "", image, link)
                    if result:
                        count += 1
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ SKUD-System: {e}")
    
    logger.info(f"üìä SKUD-System: –¥–æ–±–∞–≤–ª–µ–Ω–æ {count} –Ω–æ–≤–æ—Å—Ç–µ–π")
    return count


# ============ –ö–ê–ó–ê–•–°–¢–ê–ù–°–ö–ò–ï –°–ê–ô–¢–´ ============

async def parse_orion_m2m():
    """Orion M2M"""
    logger.info("üá∞üáø –ü–∞—Ä—Å–∏–Ω–≥ Orion M2M...")
    url = "https://orion-m2m.kz/news/2025/"
    html = await fetch_html(url)
    if not html:
        return 0
    
    soup = BeautifulSoup(html, 'html.parser')
    news_items = soup.select('.news-item, article, .post')[:5]
    
    count = 0
    for item in news_items:
        try:
            title_el = item.select_one('h2, h3, .title, a')
            title = title_el.get_text(strip=True) if title_el else None
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–µ–ª–µ—Ñ–æ–Ω—ã –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
            if not title or len(title) < 15 or title.startswith('+7'):
                continue
            
            link_el = item.select_one('a[href]')
            link = link_el['href'] if link_el else None
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è —Å—Å—ã–ª–∫–∏ - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º tel: –∏ mailto:
            if link:
                if link.startswith('tel:') or link.startswith('mailto:'):
                    continue
                if not link.startswith('http'):
                    link = f"https://orion-m2m.kz{link}"
            
            content_el = item.select_one('p, .description')
            content = content_el.get_text(strip=True) if content_el else title
            
            img_el = item.select_one('img')
            image = img_el.get('src') if img_el else None
            
            if title and len(title) > 10:
                existing = supabase.table("news").select("id").eq("title", title).execute()
                if not existing.data:
                    result = await save_news("Orion M2M", title, content or "", image, link)
                    if result:
                        count += 1
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Orion M2M: {e}")
    
    logger.info(f"üìä Orion M2M: –¥–æ–±–∞–≤–ª–µ–Ω–æ {count} –Ω–æ–≤–æ—Å—Ç–µ–π")
    return count


async def parse_intant():
    """Intant"""
    logger.info("üá∞üáø –ü–∞—Ä—Å–∏–Ω–≥ Intant...")
    url = "https://intant.kz/news/"
    html = await fetch_html(url)
    if not html:
        return 0
    
    soup = BeautifulSoup(html, 'html.parser')
    news_items = soup.select('.news-item, article, .post')[:5]
    
    count = 0
    for item in news_items:
        try:
            title_el = item.select_one('h2, h3, .title, a')
            title = title_el.get_text(strip=True) if title_el else None
            
            link_el = item.select_one('a[href]')
            link = link_el['href'] if link_el else None
            if link and not link.startswith('http'):
                link = f"https://intant.kz{link}"
            
            content_el = item.select_one('p, .description')
            content = content_el.get_text(strip=True) if content_el else title
            
            img_el = item.select_one('img')
            image = img_el.get('src') if img_el else None
            
            if title and len(title) > 10:
                existing = supabase.table("news").select("id").eq("title", title).execute()
                if not existing.data:
                    result = await save_news("Intant", title, content or "", image, link)
                    if result:
                        count += 1
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Intant: {e}")
    
    logger.info(f"üìä Intant: –¥–æ–±–∞–≤–ª–µ–Ω–æ {count} –Ω–æ–≤–æ—Å—Ç–µ–π")
    return count


async def parse_inform_kz():
    """Inform.kz (–≤–∏–¥–µ–æ–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ)"""
    logger.info("üá∞üáø –ü–∞—Ä—Å–∏–Ω–≥ Inform.kz...")
    url = "https://www.inform.kz/tag/videonablyudenie"
    html = await fetch_html(url)
    if not html:
        return 0
    
    soup = BeautifulSoup(html, 'html.parser')
    news_items = soup.select('article, .news-item, .post')[:5]
    
    count = 0
    for item in news_items:
        try:
            title_el = item.select_one('h2, h3, .title, a')
            title = title_el.get_text(strip=True) if title_el else None
            
            link_el = item.select_one('a[href]')
            link = link_el['href'] if link_el else None
            if link and not link.startswith('http'):
                link = f"https://www.inform.kz{link}"
            
            content_el = item.select_one('p, .description, .excerpt')
            content = content_el.get_text(strip=True) if content_el else title
            
            img_el = item.select_one('img')
            image = img_el.get('src') if img_el else None
            
            if title and len(title) > 10:
                existing = supabase.table("news").select("id").eq("title", title).execute()
                if not existing.data:
                    result = await save_news("Inform.kz", title, content or "", image, link)
                    if result:
                        count += 1
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Inform.kz: {e}")
    
    logger.info(f"üìä Inform.kz: –¥–æ–±–∞–≤–ª–µ–Ω–æ {count} –Ω–æ–≤–æ—Å—Ç–µ–π")
    return count


# ============ –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –ü–ê–†–°–ò–ù–ì–ê ============

async def parse_all_sites():
    """–ü–∞—Ä—Å–∏—Ç –≤—Å–µ 21 –∏—Å—Ç–æ—á–Ω–∏–∫"""
    logger.info("üîÑ –ù–∞—á–∏–Ω–∞—é –ø–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö —Å–∞–π—Ç–æ–≤...")
    
    total_count = 0
    
    # –ó–∞—Ä—É–±–µ–∂–Ω—ã–µ
    total_count += await parse_hikvision()
    total_count += await parse_dahua()
    total_count += await parse_axis()
    total_count += await parse_bosch()
    total_count += await parse_siemens()
    total_count += await parse_hochiki()
    total_count += await parse_hanwha()
    total_count += await parse_cloudflare()
    
    # –†–æ—Å—Å–∏–π—Å–∫–∏–µ
    total_count += await parse_bolid()
    total_count += await parse_perco()
    total_count += await parse_rubezh()
    total_count += await parse_rgsec()
    total_count += await parse_dssl()
    total_count += await parse_skud_system()
    
    # –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω—Å–∫–∏–µ
    total_count += await parse_orion_m2m()
    total_count += await parse_intant()
    total_count += await parse_inform_kz()
    
    logger.info(f"‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö —Å–∞–π—Ç–æ–≤ –∑–∞–≤–µ—Ä—à—ë–Ω! –î–æ–±–∞–≤–ª–µ–Ω–æ {total_count} –Ω–æ–≤–æ—Å—Ç–µ–π")


async def scheduled_parsing(context: ContextTypes.DEFAULT_TYPE):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –ø–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é"""
    logger.info("‚è∞ –ó–∞–ø—É—Å–∫ –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞...")
    await parse_all_sites()


# ============ MAIN ============
def main():
    """–ó–∞–ø—É—Å–∫ –±–æ—Ç–∞"""
    app = Application.builder().token(TELEGRAM_TOKEN).build()
    
    # –ö–æ–º–∞–Ω–¥—ã
    app.add_handler(CommandHandler("start", start_command))
    app.add_handler(CommandHandler("parse", parse_command))
    app.add_handler(CommandHandler("list", list_command))
    app.add_handler(CommandHandler("delete", delete_command))
    app.add_handler(CommandHandler("stats", stats_command))
    app.add_handler(CommandHandler("sources", sources_command))
    
    # –û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å–æ–æ–±—â–µ–Ω–∏–π
    app.add_handler(MessageHandler(filters.ALL, handle_message))
    
    # –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ (–∫–∞–∂–¥—ã–µ 2 —á–∞—Å–∞)
    job_queue = app.job_queue
    job_queue.run_repeating(scheduled_parsing, interval=7200, first=10)
    
    logger.info("üöÄ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω!")
    logger.info("üåê –ü–∞—Ä—Å–∏–Ω–≥ 21 –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –æ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏")
    logger.info("üîç –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ ~90 –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º")
    logger.info("üìã –ü–µ—Ä–≤—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ 10 —Å–µ–∫—É–Ω–¥, –∑–∞—Ç–µ–º –∫–∞–∂–¥—ã–µ 2 —á–∞—Å–∞")
    app.run_polling(allowed_updates=Update.ALL_TYPES)


if __name__ == "__main__":
    main()
