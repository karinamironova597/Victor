import asyncio
import logging
from datetime import datetime
from typing import Optional
import aiohttp
from bs4 import BeautifulSoup
from telegram import Update
from telegram.ext import Application, MessageHandler, CommandHandler, filters, ContextTypes
from supabase import create_client, Client

# ============ –ù–ê–°–¢–†–û–ô–ö–ò ============
TELEGRAM_TOKEN = "8540569762:AAFnvJS9v7P6mlfhK1sfGSpQ_nIsY2bbM6s"
SUPABASE_URL = "https://dqpqhvaikapnnmablvxh.supabase.co"
SUPABASE_KEY = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImRxcHFodmFpa2Fwbm5tYWJsdnhoIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc2OTQ5MzE3MywiZXhwIjoyMDg1MDY5MTczfQ.bNsJE5orouvDoCHa4q9p0FwbaMDgsLuhXSIGmN9h7Qc"
CHANNEL_ID = "@iqsafety_news"

# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

# Supabase –∫–ª–∏–µ–Ω—Ç
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)


# ============ –°–û–•–†–ê–ù–ï–ù–ò–ï –í –ë–ê–ó–£ ============
async def save_news(source: str, title: str, content: str, image_url: str = None, post_url: str = None):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–æ–≤–æ—Å—Ç—å –≤ Supabase"""
    try:
        data = {
            "source": source,
            "title": title,
            "content": content,
            "image_url": image_url,
            "post_url": post_url,
            "deleted": False  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –Ω–µ —É–¥–∞–ª–µ–Ω–∞
        }
        result = supabase.table("news").insert(data).execute()
        logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {title[:50]}...")
        return result
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")
        return None


# ============ –ö–û–ú–ê–ù–î–´ –ë–û–¢–ê ============
async def start_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–ö–æ–º–∞–Ω–¥–∞ /start"""
    await update.message.reply_text(
        "ü§ñ –ë–æ—Ç –Ω–æ–≤–æ—Å—Ç–µ–π IQ Safety –∑–∞–ø—É—â–µ–Ω!\n\n"
        "üìã –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã:\n"
        "/parse - –ó–∞–ø—É—Å—Ç–∏—Ç—å –ø–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–µ–π –≤—Ä—É—á–Ω—É—é\n"
        "/list - –ü–æ–∫–∞–∑–∞—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ —Å ID\n"
        "/delete <ID> - –£–¥–∞–ª–∏—Ç—å –Ω–æ–≤–æ—Å—Ç—å –ø–æ ID\n"
        "/stats - –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π"
    )


async def parse_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–ö–æ–º–∞–Ω–¥–∞ /parse - –∑–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤—Ä—É—á–Ω—É—é"""
    await update.message.reply_text("üîÑ –ó–∞–ø—É—Å–∫–∞—é –ø–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–µ–π...")
    
    try:
        await parse_all_sites()
        await update.message.reply_text("‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à—ë–Ω! –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö.")
    except Exception as e:
        await update.message.reply_text(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
        logger.error(f"–û—à–∏–±–∫–∞ –≤ parse_command: {e}")


async def list_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–ö–æ–º–∞–Ω–¥–∞ /list - –ø–æ–∫–∞–∑–∞—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏"""
    try:
        response = supabase.table('news').select('id, title, source, deleted').order('created_at', desc=True).limit(15).execute()
        
        if not response.data:
            await update.message.reply_text("üì≠ –ù–æ–≤–æ—Å—Ç–µ–π –ø–æ–∫–∞ –Ω–µ—Ç")
            return
        
        message = "üì∞ –ü–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏:\n\n"
        for news in response.data:
            deleted_mark = " ‚ùå" if news.get('deleted') else ""
            source = news.get('source', 'N/A')
            title = news.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')[:60]
            message += f"ID: {news['id']} | {source}\n{title}...{deleted_mark}\n\n"
        
        await update.message.reply_text(message)
    except Exception as e:
        await update.message.reply_text(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        logger.error(f"–û—à–∏–±–∫–∞ –≤ list_command: {e}")


async def delete_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–ö–æ–º–∞–Ω–¥–∞ /delete <ID> - –ø–æ–º–µ—Ç–∏—Ç—å –Ω–æ–≤–æ—Å—Ç—å –∫–∞–∫ —É–¥–∞–ª—ë–Ω–Ω—É—é"""
    if not context.args:
        await update.message.reply_text("‚ùå –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: /delete <ID –Ω–æ–≤–æ—Å—Ç–∏>\n\n–ü—Ä–∏–º–µ—Ä: /delete 5")
        return
    
    try:
        news_id = int(context.args[0])
        
        # –ü–æ–º–µ—á–∞–µ–º –∫–∞–∫ —É–¥–∞–ª—ë–Ω–Ω—É—é
        response = supabase.table('news').update({
            'deleted': True
        }).eq('id', news_id).execute()
        
        if response.data:
            await update.message.reply_text(f"‚úÖ –ù–æ–≤–æ—Å—Ç—å #{news_id} —É–¥–∞–ª–µ–Ω–∞ –∏–∑ –≤–∏–¥–∂–µ—Ç–∞")
        else:
            await update.message.reply_text(f"‚ùå –ù–æ–≤–æ—Å—Ç—å #{news_id} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
    except ValueError:
        await update.message.reply_text("‚ùå ID –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —á–∏—Å–ª–æ–º!")
    except Exception as e:
        await update.message.reply_text(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        logger.error(f"–û—à–∏–±–∫–∞ –≤ delete_command: {e}")


async def stats_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–ö–æ–º–∞–Ω–¥–∞ /stats - —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π"""
    try:
        all_news = supabase.table('news').select('id, source, deleted').execute()
        
        total = len(all_news.data)
        deleted = len([n for n in all_news.data if n.get('deleted')])
        active = total - deleted
        
        # –ü–æ–¥—Å—á—ë—Ç –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
        sources = {}
        for news in all_news.data:
            source = news.get('source', 'Unknown')
            sources[source] = sources.get(source, 0) + 1
        
        message = f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π:\n\n"
        message += f"üì∞ –í—Å–µ–≥–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {total}\n"
        message += f"‚úÖ –ê–∫—Ç–∏–≤–Ω—ã—Ö: {active}\n"
        message += f"‚ùå –£–¥–∞–ª—ë–Ω–Ω—ã—Ö: {deleted}\n\n"
        message += "üìç –ü–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º:\n"
        for source, count in sorted(sources.items(), key=lambda x: x[1], reverse=True):
            message += f"  ‚Ä¢ {source}: {count}\n"
        
        await update.message.reply_text(message)
    except Exception as e:
        await update.message.reply_text(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        logger.error(f"–û—à–∏–±–∫–∞ –≤ stats_command: {e}")


# ============ TELEGRAM HANDLER ============
async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ –≤—Ö–æ–¥—è—â–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è"""
    message = update.message or update.channel_post
    if not message:
        return
    
    # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç
    text = message.text or message.caption or ""
    
    # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∫–æ–º–∞–Ω–¥—ã
    if text.startswith('/'):
        return
    
    # –ü–æ–ª—É—á–∞–µ–º —Ñ–æ—Ç–æ
    image_url = None
    if message.photo:
        photo = message.photo[-1]  # –°–∞–º–æ–µ –±–æ–ª—å—à–æ–µ —Ñ–æ—Ç–æ
        file = await context.bot.get_file(photo.file_id)
        image_url = file.file_path
    
    # –°—Å—ã–ª–∫–∞ –Ω–∞ –ø–æ—Å—Ç (–µ—Å–ª–∏ —ç—Ç–æ –∏–∑ –∫–∞–Ω–∞–ª–∞)
    post_url = None
    if update.channel_post:
        post_url = f"https://t.me/{CHANNEL_ID.replace('@', '')}/{message.message_id}"
    
    # –ó–∞–≥–æ–ª–æ–≤–æ–∫ ‚Äî –ø–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ –∏–ª–∏ –ø–µ—Ä–≤—ã–µ 100 —Å–∏–º–≤–æ–ª–æ–≤
    title = text.split('\n')[0][:100] if text else "–ù–æ–≤–æ—Å—Ç—å IQ Safety"
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    result = await save_news(
        source="IQ Safety",
        title=title,
        content=text,
        image_url=image_url,
        post_url=post_url
    )
    
    # –ï—Å–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ –∏–∑ –ª–∏—á–Ω–æ–≥–æ —á–∞—Ç–∞ - –ø—É–±–ª–∏–∫—É–µ–º –≤ –∫–∞–Ω–∞–ª
    if update.message and result:
        try:
            if image_url:
                await context.bot.send_photo(
                    chat_id=CHANNEL_ID,
                    photo=image_url,
                    caption=text
                )
            else:
                await context.bot.send_message(
                    chat_id=CHANNEL_ID,
                    text=text
                )
            await update.message.reply_text("‚úÖ –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ –≤ –∫–∞–Ω–∞–ª–µ!")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –≤ –∫–∞–Ω–∞–ª: {e}")
            await update.message.reply_text(f"‚ùå –û—à–∏–±–∫–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {e}")


# ============ –ü–ê–†–°–ò–ù–ì –°–ê–ô–¢–û–í ============
async def fetch_html(url: str) -> Optional[str]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(url, timeout=30) as response:
                if response.status == 200:
                    return await response.text()
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}: {e}")
    return None


async def parse_perco():
    """–ü–∞—Ä—Å–∏—Ç –Ω–æ–≤–æ—Å—Ç–∏ —Å PERCO"""
    logger.info("–ü–∞—Ä—Å–∏–Ω–≥ PERCO...")
    url = "https://www.perco.ru/news/"
    html = await fetch_html(url)
    if not html:
        return
    
    soup = BeautifulSoup(html, 'html.parser')
    news_items = soup.select('.news-item, .news-list-item, article')[:5]
    
    for item in news_items:
        try:
            title_el = item.select_one('h2, h3, .title, a')
            title = title_el.get_text(strip=True) if title_el else None
            
            link_el = item.select_one('a[href]')
            link = link_el['href'] if link_el else None
            if link and not link.startswith('http'):
                link = f"https://www.perco.ru{link}"
            
            content_el = item.select_one('p, .description, .text')
            content = content_el.get_text(strip=True) if content_el else ""
            
            img_el = item.select_one('img')
            image = img_el['src'] if img_el else None
            if image and not image.startswith('http'):
                image = f"https://www.perco.ru{image}"
            
            if title:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Ç–∞–∫–æ–π –Ω–æ–≤–æ—Å—Ç–∏ –µ—â—ë –Ω–µ—Ç
                existing = supabase.table("news").select("id").eq("title", title).execute()
                if not existing.data:
                    await save_news("PERCO", title, content, image, link)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ PERCO: {e}")


async def parse_hikvision():
    """–ü–∞—Ä—Å–∏—Ç –Ω–æ–≤–æ—Å—Ç–∏ —Å Hikvision"""
    logger.info("–ü–∞—Ä—Å–∏–Ω–≥ Hikvision...")
    url = "https://www.hikvision.com/ru/newsroom/latest-news/"
    html = await fetch_html(url)
    if not html:
        return
    
    soup = BeautifulSoup(html, 'html.parser')
    news_items = soup.select('.news-item, .news-card, article, .item')[:5]
    
    for item in news_items:
        try:
            title_el = item.select_one('h2, h3, .title, a')
            title = title_el.get_text(strip=True) if title_el else None
            
            link_el = item.select_one('a[href]')
            link = link_el['href'] if link_el else None
            if link and not link.startswith('http'):
                link = f"https://www.hikvision.com{link}"
            
            content_el = item.select_one('p, .description')
            content = content_el.get_text(strip=True) if content_el else ""
            
            img_el = item.select_one('img')
            image = img_el.get('src') or img_el.get('data-src') if img_el else None
            
            if title:
                existing = supabase.table("news").select("id").eq("title", title).execute()
                if not existing.data:
                    await save_news("Hikvision", title, content, image, link)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Hikvision: {e}")


async def parse_tbloc():
    """–ü–∞—Ä—Å–∏—Ç –Ω–æ–≤–æ—Å—Ç–∏ —Å TBLOC"""
    logger.info("–ü–∞—Ä—Å–∏–Ω–≥ TBLOC...")
    url = "https://t-bloc.ru/news/"
    html = await fetch_html(url)
    if not html:
        return
    
    soup = BeautifulSoup(html, 'html.parser')
    news_items = soup.select('.news-item, article, .post')[:5]
    
    for item in news_items:
        try:
            title_el = item.select_one('h2, h3, .title, a')
            title = title_el.get_text(strip=True) if title_el else None
            
            link_el = item.select_one('a[href]')
            link = link_el['href'] if link_el else None
            if link and not link.startswith('http'):
                link = f"https://t-bloc.ru{link}"
            
            content_el = item.select_one('p, .description')
            content = content_el.get_text(strip=True) if content_el else ""
            
            img_el = item.select_one('img')
            image = img_el['src'] if img_el else None
            
            if title:
                existing = supabase.table("news").select("id").eq("title", title).execute()
                if not existing.data:
                    await save_news("TBLOC", title, content, image, link)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ TBLOC: {e}")


async def parse_zkteco():
    """–ü–∞—Ä—Å–∏—Ç –Ω–æ–≤–æ—Å—Ç–∏ —Å ZKTeco"""
    logger.info("–ü–∞—Ä—Å–∏–Ω–≥ ZKTeco...")
    url = "https://www.zkteco.ru/news/"
    html = await fetch_html(url)
    if not html:
        return
    
    soup = BeautifulSoup(html, 'html.parser')
    news_items = soup.select('.news-item, article, .post, .news-card')[:5]
    
    for item in news_items:
        try:
            title_el = item.select_one('h2, h3, .title, a')
            title = title_el.get_text(strip=True) if title_el else None
            
            link_el = item.select_one('a[href]')
            link = link_el['href'] if link_el else None
            if link and not link.startswith('http'):
                link = f"https://www.zkteco.ru{link}"
            
            content_el = item.select_one('p, .description')
            content = content_el.get_text(strip=True) if content_el else ""
            
            img_el = item.select_one('img')
            image = img_el.get('src') or img_el.get('data-src') if img_el else None
            
            if title:
                existing = supabase.table("news").select("id").eq("title", title).execute()
                if not existing.data:
                    await save_news("ZKTeco", title, content, image, link)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ ZKTeco: {e}")


async def parse_dahua():
    """–ü–∞—Ä—Å–∏—Ç –Ω–æ–≤–æ—Å—Ç–∏ —Å Dahua"""
    logger.info("–ü–∞—Ä—Å–∏–Ω–≥ Dahua...")
    url = "https://www.dahuasecurity.com/ru/newsEvents/news"
    html = await fetch_html(url)
    if not html:
        return
    
    soup = BeautifulSoup(html, 'html.parser')
    news_items = soup.select('.news-item, article, .news-list li, .item')[:5]
    
    for item in news_items:
        try:
            title_el = item.select_one('h2, h3, .title, a')
            title = title_el.get_text(strip=True) if title_el else None
            
            link_el = item.select_one('a[href]')
            link = link_el['href'] if link_el else None
            if link and not link.startswith('http'):
                link = f"https://www.dahuasecurity.com{link}"
            
            content_el = item.select_one('p, .description, .text')
            content = content_el.get_text(strip=True) if content_el else ""
            
            img_el = item.select_one('img')
            image = img_el.get('src') if img_el else None
            
            if title:
                existing = supabase.table("news").select("id").eq("title", title).execute()
                if not existing.data:
                    await save_news("Dahua", title, content, image, link)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Dahua: {e}")


async def parse_axis():
    """–ü–∞—Ä—Å–∏—Ç –Ω–æ–≤–æ—Å—Ç–∏ —Å Axis Communications"""
    logger.info("–ü–∞—Ä—Å–∏–Ω–≥ Axis...")
    url = "https://www.axis.com/ru-ru/about-axis/news"
    html = await fetch_html(url)
    if not html:
        return
    
    soup = BeautifulSoup(html, 'html.parser')
    news_items = soup.select('.news-item, article, .news-card, .item')[:5]
    
    for item in news_items:
        try:
            title_el = item.select_one('h2, h3, .title, a')
            title = title_el.get_text(strip=True) if title_el else None
            
            link_el = item.select_one('a[href]')
            link = link_el['href'] if link_el else None
            if link and not link.startswith('http'):
                link = f"https://www.axis.com{link}"
            
            content_el = item.select_one('p, .description')
            content = content_el.get_text(strip=True) if content_el else ""
            
            img_el = item.select_one('img')
            image = img_el.get('src') if img_el else None
            
            if title:
                existing = supabase.table("news").select("id").eq("title", title).execute()
                if not existing.data:
                    await save_news("Axis", title, content, image, link)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Axis: {e}")


async def parse_bolid():
    """–ü–∞—Ä—Å–∏—Ç –Ω–æ–≤–æ—Å—Ç–∏ —Å –ë–æ–ª–∏–¥ (—Ä–æ—Å—Å–∏–π—Å–∫–∏–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å)"""
    logger.info("–ü–∞—Ä—Å–∏–Ω–≥ –ë–æ–ª–∏–¥...")
    url = "https://bolid.ru/company/news/"
    html = await fetch_html(url)
    if not html:
        return
    
    soup = BeautifulSoup(html, 'html.parser')
    news_items = soup.select('.news-item, article, .news-list-item, .item')[:5]
    
    for item in news_items:
        try:
            title_el = item.select_one('h2, h3, .title, a')
            title = title_el.get_text(strip=True) if title_el else None
            
            link_el = item.select_one('a[href]')
            link = link_el['href'] if link_el else None
            if link and not link.startswith('http'):
                link = f"https://bolid.ru{link}"
            
            content_el = item.select_one('p, .description, .anons')
            content = content_el.get_text(strip=True) if content_el else ""
            
            img_el = item.select_one('img')
            image = img_el.get('src') if img_el else None
            if image and not image.startswith('http'):
                image = f"https://bolid.ru{image}"
            
            if title:
                existing = supabase.table("news").select("id").eq("title", title).execute()
                if not existing.data:
                    await save_news("–ë–æ–ª–∏–¥", title, content, image, link)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –ë–æ–ª–∏–¥: {e}")


async def parse_securitymedia():
    """–ü–∞—Ä—Å–∏—Ç –Ω–æ–≤–æ—Å—Ç–∏ —Å Security Media (–æ—Ç—Ä–∞—Å–ª–µ–≤–æ–π –ø–æ—Ä—Ç–∞–ª)"""
    logger.info("–ü–∞—Ä—Å–∏–Ω–≥ SecurityMedia...")
    url = "https://www.securitymedia.ru/news/"
    html = await fetch_html(url)
    if not html:
        return
    
    soup = BeautifulSoup(html, 'html.parser')
    news_items = soup.select('.news-item, article, .post, .item')[:5]
    
    for item in news_items:
        try:
            title_el = item.select_one('h2, h3, .title, a')
            title = title_el.get_text(strip=True) if title_el else None
            
            link_el = item.select_one('a[href]')
            link = link_el['href'] if link_el else None
            if link and not link.startswith('http'):
                link = f"https://www.securitymedia.ru{link}"
            
            content_el = item.select_one('p, .description, .anons')
            content = content_el.get_text(strip=True) if content_el else ""
            
            img_el = item.select_one('img')
            image = img_el.get('src') if img_el else None
            
            if title:
                existing = supabase.table("news").select("id").eq("title", title).execute()
                if not existing.data:
                    await save_news("–û—Ç—Ä–∞—Å–ª—å", title, content, image, link)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ SecurityMedia: {e}")


async def parse_all_sites():
    """–ü–∞—Ä—Å–∏—Ç –≤—Å–µ —Å–∞–π—Ç—ã"""
    logger.info("üîÑ –ù–∞—á–∏–Ω–∞—é –ø–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö —Å–∞–π—Ç–æ–≤...")
    await parse_perco()
    await parse_hikvision()
    await parse_tbloc()
    await parse_zkteco()
    await parse_dahua()
    await parse_axis()
    await parse_bolid()
    await parse_securitymedia()
    logger.info("‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö —Å–∞–π—Ç–æ–≤ –∑–∞–≤–µ—Ä—à—ë–Ω")


async def scheduled_parsing(context: ContextTypes.DEFAULT_TYPE):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –ø–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é"""
    logger.info("‚è∞ –ó–∞–ø—É—Å–∫ –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞...")
    await parse_all_sites()


# ============ MAIN ============
def main():
    """–ó–∞–ø—É—Å–∫ –±–æ—Ç–∞"""
    # –°–æ–∑–¥–∞—ë–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
    app = Application.builder().token(TELEGRAM_TOKEN).build()
    
    # –ö–æ–º–∞–Ω–¥—ã
    app.add_handler(CommandHandler("start", start_command))
    app.add_handler(CommandHandler("parse", parse_command))
    app.add_handler(CommandHandler("list", list_command))
    app.add_handler(CommandHandler("delete", delete_command))
    app.add_handler(CommandHandler("stats", stats_command))
    
    # –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –≤—Å–µ—Ö —Å–æ–æ–±—â–µ–Ω–∏–π (–∏ –∏–∑ –ª–∏—á–Ω–æ–≥–æ —á–∞—Ç–∞, –∏ –∏–∑ –∫–∞–Ω–∞–ª–∞)
    app.add_handler(MessageHandler(filters.ALL, handle_message))
    
    # –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ (–∫–∞–∂–¥—ã–µ 2 —á–∞—Å–∞)
    job_queue = app.job_queue
    job_queue.run_repeating(scheduled_parsing, interval=7200, first=10)
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º
    logger.info("üöÄ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω!")
    logger.info("üìã –ü–µ—Ä–≤—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ 10 —Å–µ–∫—É–Ω–¥, –∑–∞—Ç–µ–º –∫–∞–∂–¥—ã–µ 2 —á–∞—Å–∞")
    app.run_polling(allowed_updates=Update.ALL_TYPES)


if __name__ == "__main__":
    main()
