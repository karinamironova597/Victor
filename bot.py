import asyncio
import logging
from datetime import datetime
from typing import Optional
import aiohttp
from bs4 import BeautifulSoup
from telegram import Update
from telegram.ext import Application, MessageHandler, CommandHandler, filters, ContextTypes
from supabase import create_client, Client
import re

# ============ –ù–ê–°–¢–†–û–ô–ö–ò ============
TELEGRAM_TOKEN = "8540569762:AAFnvJS9v7P6mlfhK1sfGSpQ_nIsY2bbM6s"
SUPABASE_URL = "https://dqpqhvaikapnnmablvxh.supabase.co"
SUPABASE_KEY = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImRxcHFodmFpa2Fwbm5tYWJsdnhoIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc2OTQ5MzE3MywiZXhwIjoyMDg1MDY5MTczfQ.bNsJE5orouvDoCHa4q9p0FwbaMDgsLuhXSIGmN9h7Qc"
CHANNEL_ID = "@iqsafety_news"

# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

# Supabase –∫–ª–∏–µ–Ω—Ç
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# ============ –ö–õ–Æ–ß–ï–í–´–ï –°–õ–û–í–ê –î–õ–Ø –§–ò–õ–¨–¢–†–ê–¶–ò–ò ============
KEYWORDS = [
    # –í–∏–¥–µ–æ–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ
    r'–≤–∏–¥–µ–æ–Ω–∞–±–ª—é–¥–µ–Ω', r'–∫–∞–º–µ—Ä', r'cctv', r'ip-–∫–∞–º–µ—Ä', r'–≤–∏–¥–µ–æ—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ç–æ—Ä',
    r'nvr', r'dvr', r'–≤–∏–¥–µ–æ–∞–Ω–∞–ª–∏—Ç–∏–∫',
    
    # –°–ö–£–î
    r'—Å–∫—É–¥', r'–∫–æ–Ω—Ç—Ä–æ–ª—å –¥–æ—Å—Ç—É–ø', r'—Ç—É—Ä–Ω–∏–∫–µ—Ç', r'—à–ª–∞–≥–±–∞—É–º', r'–¥–æ–º–æ—Ñ–æ–Ω',
    r'–≤–∏–¥–µ–æ–¥–æ–º–æ—Ñ–æ–Ω', r'—Å—á–∏—Ç—ã–≤–∞—Ç–µ–ª—å', r'–∫–∞—Ä—Ç-—Ä–∏–¥–µ—Ä', r'—ç–ª–µ–∫—Ç—Ä–æ–∑–∞–º–æ–∫',
    
    # –ü–æ–∂–∞—Ä–Ω–∞—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å
    r'–ø–æ–∂–∞—Ä–Ω', r'–æ–≥–Ω–µ—Ç—É—à–∏—Ç–µ–ª', r'–¥–∞—Ç—á–∏–∫ –¥—ã–º', r'–ø–æ–∂–∞—Ä–æ—Ç—É—à–µ–Ω',
    r'—Å–ø—Ä–∏–Ω–∫–ª–µ—Ä', r'–æ–ø—Å', r'–∞—É–ø—Ç', r'–ø–æ–∂–∞—Ä–Ω–∞—è —Å–∏–≥–Ω–∞–ª–∏–∑–∞—Ü',
    
    # –û—Ö—Ä–∞–Ω–Ω–∞—è —Å–∏–≥–Ω–∞–ª–∏–∑–∞—Ü–∏—è
    r'—Å–∏–≥–Ω–∞–ª–∏–∑–∞—Ü', r'–æ—Ö—Ä–∞–Ω', r'–¥–∞—Ç—á–∏–∫ –¥–≤–∏–∂–µ–Ω', r'–¥–∞—Ç—á–∏–∫ —Ä–∞–∑–±–∏—Ç',
    r'–ø–µ—Ä–∏–º–µ—Ç—Ä', r'–æ–≥—Ä–∞–∂–¥–µ–Ω–∏–µ', r'—Ç—Ä–µ–≤–æ–∂–Ω',
    
    # –ë–∏–æ–º–µ—Ç—Ä–∏—è
    r'–±–∏–æ–º–µ—Ç—Ä', r'—Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –ª–∏—Ü', r'–æ—Ç–ø–µ—á–∞—Ç–æ–∫', r'—Å–∫–∞–Ω–µ—Ä –ª–∏—Ü',
    r'face recognition', r'–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü', r'–∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü',
    
    # –ë—Ä–µ–Ω–¥—ã –º–∏—Ä–æ–≤—ã–µ
    r'hikvision', r'dahua', r'axis', r'bosch', r'siemens', r'hanwha',
    r'honeywell', r'hochiki', r'schneider electric', r'panasonic',
    
    # –ë—Ä–µ–Ω–¥—ã —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ/–°–ù–ì
    r'–±–æ–ª–∏–¥', r'—Ä—É–±–µ–∂', r'perco', r'parsec', r'–æ—Ä–∏–æ–Ω', r'itv', r'—Å–∏–≥–º–∞',
    r'smartec', r'beward', r'dssl', r'fort', r'tantos',
    
    # –û–±—â–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
    r'–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç', r'–æ—Ö—Ä–∞–Ω–∞', r'security', r'—Å–∏—Å—Ç–µ–º–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç',
    r'–∫–æ–º–ø–ª–µ–∫—Å –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç', r'–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º',
    
    # –£–º–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏
    r'—É–º–Ω—ã–π –¥–æ–º', r'smart home', r'iot', r'–∏–Ω—Ç–µ—Ä–Ω–µ—Ç –≤–µ—â–µ–π',
    r'–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è', r'–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω',
    
    # –°–µ—Ç–∏
    r'ip-—Å–∏—Å—Ç–µ–º', r'—Å–µ—Ç–µ–≤', r'ethernet', r'poe', r'wi-fi –∫–∞–º–µ—Ä',
    r'–æ–±–ª–∞—á–Ω', r'cloud'
]

def check_keywords(text: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ"""
    if not text:
        return False
    
    text_lower = text.lower()
    
    for keyword in KEYWORDS:
        if re.search(keyword, text_lower):
            return True
    
    return False


# ============ –°–û–•–†–ê–ù–ï–ù–ò–ï –í –ë–ê–ó–£ ============
async def save_news(source: str, title: str, content: str, image_url: str = None, post_url: str = None):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–æ–≤–æ—Å—Ç—å –≤ Supabase —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
    combined_text = f"{title} {content}"
    if not check_keywords(combined_text):
        logger.info(f"‚è≠Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω–æ (–Ω–µ—Ç –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤): {title[:50]}...")
        return None
    
    try:
        data = {
            "source": source,
            "title": title,
            "content": content,
            "image_url": image_url,
            "post_url": post_url,
            "deleted": False
        }
        result = supabase.table("news").insert(data).execute()
        logger.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {title[:50]}...")
        return result
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")
        return None


# ============ –ö–û–ú–ê–ù–î–´ –ë–û–¢–ê ============
async def start_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–ö–æ–º–∞–Ω–¥–∞ /start"""
    await update.message.reply_text(
        "ü§ñ –ë–æ—Ç –Ω–æ–≤–æ—Å—Ç–µ–π IQ Safety –∑–∞–ø—É—â–µ–Ω!\n\n"
        "üìã –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã:\n"
        "/parse - –ó–∞–ø—É—Å—Ç–∏—Ç—å –ø–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–µ–π –≤—Ä—É—á–Ω—É—é\n"
        "/list - –ü–æ–∫–∞–∑–∞—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ —Å ID\n"
        "/delete <ID> - –£–¥–∞–ª–∏—Ç—å –Ω–æ–≤–æ—Å—Ç—å –ø–æ ID\n"
        "/stats - –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π\n"
        "/sources - –°–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"
    )


async def parse_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–ö–æ–º–∞–Ω–¥–∞ /parse - –∑–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤—Ä—É—á–Ω—É—é"""
    await update.message.reply_text("üîÑ –ó–∞–ø—É—Å–∫–∞—é –ø–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–µ–π...")
    
    try:
        await parse_all_sites()
        await update.message.reply_text("‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à—ë–Ω! –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö.")
    except Exception as e:
        await update.message.reply_text(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
        logger.error(f"–û—à–∏–±–∫–∞ –≤ parse_command: {e}")


async def list_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–ö–æ–º–∞–Ω–¥–∞ /list - –ø–æ–∫–∞–∑–∞—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏"""
    try:
        response = supabase.table('news').select('id, title, source, deleted').order('created_at', desc=True).limit(15).execute()
        
        if not response.data:
            await update.message.reply_text("üì≠ –ù–æ–≤–æ—Å—Ç–µ–π –ø–æ–∫–∞ –Ω–µ—Ç")
            return
        
        message = "üì∞ –ü–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏:\n\n"
        for news in response.data:
            deleted_mark = " ‚ùå" if news.get('deleted') else ""
            source = news.get('source', 'N/A')
            title = news.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')[:60]
            message += f"ID: {news['id']} | {source}\n{title}...{deleted_mark}\n\n"
        
        await update.message.reply_text(message)
    except Exception as e:
        await update.message.reply_text(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        logger.error(f"–û—à–∏–±–∫–∞ –≤ list_command: {e}")


async def delete_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–ö–æ–º–∞–Ω–¥–∞ /delete <ID> - –ø–æ–º–µ—Ç–∏—Ç—å –Ω–æ–≤–æ—Å—Ç—å –∫–∞–∫ —É–¥–∞–ª—ë–Ω–Ω—É—é"""
    if not context.args:
        await update.message.reply_text("‚ùå –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: /delete <ID –Ω–æ–≤–æ—Å—Ç–∏>\n\n–ü—Ä–∏–º–µ—Ä: /delete 5")
        return
    
    try:
        news_id = int(context.args[0])
        
        response = supabase.table('news').update({
            'deleted': True
        }).eq('id', news_id).execute()
        
        if response.data:
            await update.message.reply_text(f"‚úÖ –ù–æ–≤–æ—Å—Ç—å #{news_id} —É–¥–∞–ª–µ–Ω–∞ –∏–∑ –≤–∏–¥–∂–µ—Ç–∞")
        else:
            await update.message.reply_text(f"‚ùå –ù–æ–≤–æ—Å—Ç—å #{news_id} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
    except ValueError:
        await update.message.reply_text("‚ùå ID –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —á–∏—Å–ª–æ–º!")
    except Exception as e:
        await update.message.reply_text(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        logger.error(f"–û—à–∏–±–∫–∞ –≤ delete_command: {e}")


async def stats_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–ö–æ–º–∞–Ω–¥–∞ /stats - —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π"""
    try:
        all_news = supabase.table('news').select('id, source, deleted').execute()
        
        total = len(all_news.data)
        deleted = len([n for n in all_news.data if n.get('deleted')])
        active = total - deleted
        
        sources = {}
        for news in all_news.data:
            if not news.get('deleted'):
                source = news.get('source', 'Unknown')
                sources[source] = sources.get(source, 0) + 1
        
        message = f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π:\n\n"
        message += f"üì∞ –í—Å–µ–≥–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {total}\n"
        message += f"‚úÖ –ê–∫—Ç–∏–≤–Ω—ã—Ö: {active}\n"
        message += f"‚ùå –£–¥–∞–ª—ë–Ω–Ω—ã—Ö: {deleted}\n\n"
        message += "üìç –ê–∫—Ç–∏–≤–Ω—ã–µ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º:\n"
        for source, count in sorted(sources.items(), key=lambda x: x[1], reverse=True):
            message += f"  ‚Ä¢ {source}: {count}\n"
        
        await update.message.reply_text(message)
    except Exception as e:
        await update.message.reply_text(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        logger.error(f"–û—à–∏–±–∫–∞ –≤ stats_command: {e}")


async def sources_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–ö–æ–º–∞–Ω–¥–∞ /sources - —Å–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
    message = """üåê –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π:

üåç –ó–∞—Ä—É–±–µ–∂–Ω—ã–µ:
‚Ä¢ Hikvision
‚Ä¢ Dahua
‚Ä¢ Axis
‚Ä¢ Bosch
‚Ä¢ Siemens
‚Ä¢ Hochiki
‚Ä¢ Hanwha Vision
‚Ä¢ Cloudflare

üá∑üá∫ –†–æ—Å—Å–∏–π—Å–∫–∏–µ:
‚Ä¢ –ë–æ–ª–∏–¥
‚Ä¢ –†—É–±–µ–∂
‚Ä¢ Perco
‚Ä¢ DSSL
‚Ä¢ RGSec
‚Ä¢ SKUD-System

üá∞üáø –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω—Å–∫–∏–µ:
‚Ä¢ Orion M2M
‚Ä¢ Intant
‚Ä¢ Inform.kz

üìã –í—Å–µ–≥–æ: 21 –∏—Å—Ç–æ—á–Ω–∏–∫
üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ: –∫–∞–∂–¥—ã–µ 2 —á–∞—Å–∞
üîç –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è: ~90 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
    
    await update.message.reply_text(message)


# ============ TELEGRAM HANDLER ============
async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ –≤—Ö–æ–¥—è—â–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è"""
    message = update.message or update.channel_post
    if not message:
        return
    
    text = message.text or message.caption or ""
    
    if text.startswith('/'):
        return
    
    image_url = None
    if message.photo:
        photo = message.photo[-1]
        file = await context.bot.get_file(photo.file_id)
        image_url = file.file_path
    
    post_url = None
    if update.channel_post:
        post_url = f"https://t.me/{CHANNEL_ID.replace('@', '')}/{message.message_id}"
    
    title = text.split('\n')[0][:100] if text else "–ù–æ–≤–æ—Å—Ç—å IQ Safety"
    
    result = await save_news(
        source="IQ Safety",
        title=title,
        content=text,
        image_url=image_url,
        post_url=post_url
    )
    
    if update.message and result:
        try:
            if image_url:
                await context.bot.send_photo(
                    chat_id=CHANNEL_ID,
                    photo=image_url,
                    caption=text
                )
            else:
                await context.bot.send_message(
                    chat_id=CHANNEL_ID,
                    text=text
                )
            await update.message.reply_text("‚úÖ –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ –≤ –∫–∞–Ω–∞–ª–µ!")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –≤ –∫–∞–Ω–∞–ª: {e}")
            await update.message.reply_text(f"‚ùå –û—à–∏–±–∫–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {e}")


# ============ –ü–ê–†–°–ò–ù–ì –°–ê–ô–¢–û–í ============
async def fetch_html(url: str) -> Optional[str]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        async with aiohttp.ClientSession() as session:
            async with session.get(url, timeout=30, headers=headers) as response:
                if response.status == 200:
                    return await response.text()
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}: {e}")
    return None


# ============ –ó–ê–†–£–ë–ï–ñ–ù–´–ï –°–ê–ô–¢–´ ============

async def parse_hikvision():
    """Hikvision"""
    logger.info("üåê –ü–∞—Ä—Å–∏–Ω–≥ Hikvision...")
    url = "https://www.hikvision.com/en/newsroom/latest-news/"
    html = await fetch_html(url)
    if not html:
        return 0
    
    soup = BeautifulSoup(html, 'html.parser')
    news_items = soup.select('.news-item, article, .content-item')[:5]
    
    count = 0
    for item in news_items:
        try:
            title_el = item.select_one('h2, h3, .title, a')
            title = title_el.get_text(strip=True) if title_el else None
            
            link_el = item.select_one('a[href]')
            link = link_el['href'] if link_el else None
            if link and not link.startswith('http'):
                link = f"https://www.hikvision.com{link}"
            
            content_el = item.select_one('p, .description, .excerpt')
            content = content_el.get_text(strip=True) if content_el else title
            
            img_el = item.select_one('img')
            image = img_el.get('src') or img_el.get('data-src') if img_el else None
            
            if title and len(title) > 10:
                existing = supabase.table("news").select("id").eq("title", title).execute()
                if not existing.data:
                    result = await save_news("Hikvision", title, content or "", image, link)
                    if result:
                        count += 1
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Hikvision: {e}")
    
    logger.info(f"üìä Hikvision: –¥–æ–±–∞–≤–ª–µ–Ω–æ {count} –Ω–æ–≤–æ—Å—Ç–µ–π")
    return count


async def parse_bolid():
    """–ë–æ–ª–∏–¥"""
    logger.info("üá∑üá∫ –ü–∞—Ä—Å–∏–Ω–≥ –ë–æ–ª–∏–¥...")
    url = "https://bolid.ru/about/news/"
    html = await fetch_html(url)
    if not html:
        return 0
    
    soup = BeautifulSoup(html, 'html.parser')
    news_items = soup.select('.news-item, article, .news-list-item')[:5]
    
    count = 0
    for item in news_items:
        try:
            title_el = item.select_one('h2, h3, .title, a')
            title = title_el.get_text(strip=True) if title_el else None
            
            link_el = item.select_one('a[href]')
            link = link_el['href'] if link_el else None
            if link and not link.startswith('http'):
                link = f"https://bolid.ru{link}"
            
            content_el = item.select_one('p, .description, .anons')
            content = content_el.get_text(strip=True) if content_el else title
            
            img_el = item.select_one('img')
            image = img_el.get('src') if img_el else None
            if image and not image.startswith('http'):
                image = f"https://bolid.ru{image}"
            
            if title and len(title) > 10:
                existing = supabase.table("news").select("id").eq("title", title).execute()
                if not existing.data:
                    result = await save_news("–ë–æ–ª–∏–¥", title, content or "", image, link)
                    if result:
                        count += 1
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –ë–æ–ª–∏–¥: {e}")
    
    logger.info(f"üìä –ë–æ–ª–∏–¥: –¥–æ–±–∞–≤–ª–µ–Ω–æ {count} –Ω–æ–≤–æ—Å—Ç–µ–π")
    return count


async def parse_perco():
    """Perco"""
    logger.info("üåê –ü–∞—Ä—Å–∏–Ω–≥ Perco...")
    url = "https://www.perco.ru/novosti/"
    html = await fetch_html(url)
    if not html:
        return 0
    
    soup = BeautifulSoup(html, 'html.parser')
    news_items = soup.select('.news-item, article, .post')[:5]
    
    count = 0
    for item in news_items:
        try:
            title_el = item.select_one('h2, h3, .title, a')
            title = title_el.get_text(strip=True) if title_el else None
            
            link_el = item.select_one('a[href]')
            link = link_el['href'] if link_el else None
            if link and not link.startswith('http'):
                link = f"https://www.perco.ru{link}"
            
            content_el = item.select_one('p, .description')
            content = content_el.get_text(strip=True) if content_el else title
            
            img_el = item.select_one('img')
            image = img_el.get('src') if img_el else None
            
            if title and len(title) > 10:
                existing = supabase.table("news").select("id").eq("title", title).execute()
                if not existing.data:
                    result = await save_news("Perco", title, content or "", image, link)
                    if result:
                        count += 1
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Perco: {e}")
    
    logger.info(f"üìä Perco: –¥–æ–±–∞–≤–ª–µ–Ω–æ {count} –Ω–æ–≤–æ—Å—Ç–µ–π")
    return count


async def parse_dahua():
    """Dahua"""
    logger.info("üåê –ü–∞—Ä—Å–∏–Ω–≥ Dahua...")
    url = "https://www.dahuasecurity.com/ea/newsEvents"
    html = await fetch_html(url)
    if not html:
        return 0
    
    soup = BeautifulSoup(html, 'html.parser')
    news_items = soup.select('.news-item, article, .news-list li')[:5]
    
    count = 0
    for item in news_items:
        try:
            title_el = item.select_one('h2, h3, .title, a')
            title = title_el.get_text(strip=True) if title_el else None
            
            link_el = item.select_one('a[href]')
            link = link_el['href'] if link_el else None
            if link and not link.startswith('http'):
                link = f"https://www.dahuasecurity.com{link}"
            
            content_el = item.select_one('p, .description')
            content = content_el.get_text(strip=True) if content_el else title
            
            img_el = item.select_one('img')
            image = img_el.get('src') if img_el else None
            
            if title and len(title) > 10:
                existing = supabase.table("news").select("id").eq("title", title).execute()
                if not existing.data:
                    result = await save_news("Dahua", title, content or "", image, link)
                    if result:
                        count += 1
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Dahua: {e}")
    
    logger.info(f"üìä Dahua: –¥–æ–±–∞–≤–ª–µ–Ω–æ {count} –Ω–æ–≤–æ—Å—Ç–µ–π")
    return count


async def parse_axis():
    """Axis Communications"""
    logger.info("üåê –ü–∞—Ä—Å–∏–Ω–≥ Axis...")
    url = "https://newsroom.axis.com/"
    html = await fetch_html(url)
    if not html:
        return 0
    
    soup = BeautifulSoup(html, 'html.parser')
    news_items = soup.select('article, .news-item, .post')[:5]
    
    count = 0
    for item in news_items:
        try:
            title_el = item.select_one('h2, h3, .title, a')
            title = title_el.get_text(strip=True) if title_el else None
            
            link_el = item.select_one('a[href]')
            link = link_el['href'] if link_el else None
            if link and not link.startswith('http'):
                link = f"https://newsroom.axis.com{link}"
            
            content_el = item.select_one('p, .description')
            content = content_el.get_text(strip=True) if content_el else title
            
            img_el = item.select_one('img')
            image = img_el.get('src') if img_el else None
            
            if title and len(title) > 10:
                existing = supabase.table("news").select("id").eq("title", title).execute()
                if not existing.data:
                    result = await save_news("Axis", title, content or "", image, link)
                    if result:
                        count += 1
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Axis: {e}")
    
    logger.info(f"üìä Axis: –¥–æ–±–∞–≤–ª–µ–Ω–æ {count} –Ω–æ–≤–æ—Å—Ç–µ–π")
    return count


async def parse_bosch():
    """Bosch"""
    logger.info("üåê –ü–∞—Ä—Å–∏–Ω–≥ Bosch...")
    url = "https://www.boschbuildingtechnologies.com/lifesafetysystems/en/news-events/"
    html = await fetch_html(url)
    if not html:
        return 0
    
    soup = BeautifulSoup(html, 'html.parser')
    news_items = soup.select('article, .news-item, .content-item')[:5]
    
    count = 0
    for item in news_items:
        try:
            title_el = item.select_one('h2, h3, .title, a')
            title = title_el.get_text(strip=True) if title_el else None
            
            link_el = item.select_one('a[href]')
            link = link_el['href'] if link_el else None
            if link and not link.startswith('http'):
                link = f"https://www.boschbuildingtechnologies.com{link}"
            
            content_el = item.select_one('p, .description')
            content = content_el.get_text(strip=True) if content_el else title
            
            img_el = item.select_one('img')
            image = img_el.get('src') if img_el else None
            
            if title and len(title) > 10:
                existing = supabase.table("news").select("id").eq("title", title).execute()
                if not existing.data:
                    result = await save_news("Bosch", title, content or "", image, link)
                    if result:
                        count += 1
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Bosch: {e}")
    
    logger.info(f"üìä Bosch: –¥–æ–±–∞–≤–ª–µ–Ω–æ {count} –Ω–æ–≤–æ—Å—Ç–µ–π")
    return count


async def parse_siemens():
    """Siemens"""
    logger.info("üåê –ü–∞—Ä—Å–∏–Ω–≥ Siemens...")
    url = "https://press.siemens.com/global/en"
    html = await fetch_html(url)
    if not html:
        return 0
    
    soup = BeautifulSoup(html, 'html.parser')
    news_items = soup.select('article, .news-item, .press-release')[:5]
    
    count = 0
    for item in news_items:
        try:
            title_el = item.select_one('h2, h3, .title, a')
            title = title_el.get_text(strip=True) if title_el else None
            
            link_el = item.select_one('a[href]')
            link = link_el['href'] if link_el else None
            if link and not link.startswith('http'):
                link = f"https://press.siemens.com{link}"
            
            content_el = item.select_one('p, .description, .excerpt')
            content = content_el.get_text(strip=True) if content_el else title
            
            img_el = item.select_one('img')
            image = img_el.get('src') if img_el else None
            
            if title and len(title) > 10:
                existing = supabase.table("news").select("id").eq("title", title).execute()
                if not existing.data:
                    result = await save_news("Siemens", title, content or "", image, link)
                    if result:
                        count += 1
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Siemens: {e}")
    
    logger.info(f"üìä Siemens: –¥–æ–±–∞–≤–ª–µ–Ω–æ {count} –Ω–æ–≤–æ—Å—Ç–µ–π")
    return count


async def parse_hochiki():
    """Hochiki"""
    logger.info("üåê –ü–∞—Ä—Å–∏–Ω–≥ Hochiki...")
    url = "https://www.hochikieurope.com/news"
    html = await fetch_html(url)
    if not html:
        return 0
    
    soup = BeautifulSoup(html, 'html.parser')
    news_items = soup.select('article, .news-item, .post')[:5]
    
    count = 0
    for item in news_items:
        try:
            title_el = item.select_one('h2, h3, .title, a')
            title = title_el.get_text(strip=True) if title_el else None
            
            link_el = item.select_one('a[href]')
            link = link_el['href'] if link_el else None
            if link and not link.startswith('http'):
                link = f"https://www.hochikieurope.com{link}"
            
            content_el = item.select_one('p, .description')
            content = content_el.get_text(strip=True) if content_el else title
            
            img_el = item.select_one('img')
            image = img_el.get('src') if img_el else None
            
            if title and len(title) > 10:
                existing = supabase.table("news").select("id").eq("title", title).execute()
                if not existing.data:
                    result = await save_news("Hochiki", title, content or "", image, link)
                    if result:
                        count += 1
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Hochiki: {e}")
    
    logger.info(f"üìä Hochiki: –¥–æ–±–∞–≤–ª–µ–Ω–æ {count} –Ω–æ–≤–æ—Å—Ç–µ–π")
    return count


async def parse_rubezh():
    """–†—É–±–µ–∂"""
    logger.info("üá∑üá∫ –ü–∞—Ä—Å–∏–Ω–≥ –†—É–±–µ–∂...")
    url = "https://rubezh.ru/news"
    html = await fetch_html(url)
    if not html:
        return 0
    
    soup = BeautifulSoup(html, 'html.parser')
    news_items = soup.select('.news-item, article, .post')[:5]
    
    count = 0
    for item in news_items:
        try:
            title_el = item.select_one('h2, h3, .title, a')
            title = title_el.get_text(strip=True) if title_el else None
            
            link_el = item.select_one('a[href]')
            link = link_el['href'] if link_el else None
            if link and not link.startswith('http'):
                link = f"https://rubezh.ru{link}"
            
            content_el = item.select_one('p, .description')
            content = content_el.get_text(strip=True) if content_el else title
            
            img_el = item.select_one('img')
            image = img_el.get('src') if img_el else None
            
            if title and len(title) > 10:
                existing = supabase.table("news").select("id").eq("title", title).execute()
                if not existing.data:
                    result = await save_news("–†—É–±–µ–∂", title, content or "", image, link)
                    if result:
                        count += 1
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –†—É–±–µ–∂: {e}")
    
    logger.info(f"üìä –†—É–±–µ–∂: –¥–æ–±–∞–≤–ª–µ–Ω–æ {count} –Ω–æ–≤–æ—Å—Ç–µ–π")
    return count


async def parse_hanwha():
    """Hanwha Vision"""
    logger.info("üåê –ü–∞—Ä—Å–∏–Ω–≥ Hanwha...")
    url = "https://www.hanwhavision.com/en/news-center/news-hub/"
    html = await fetch_html(url)
    if not html:
        return 0
    
    soup = BeautifulSoup(html, 'html.parser')
    news_items = soup.select('article, .news-item, .post')[:5]
    
    count = 0
    for item in news_items:
        try:
            title_el = item.select_one('h2, h3, .title, a')
            title = title_el.get_text(strip=True) if title_el else None
            
            link_el = item.select_one('a[href]')
            link = link_el['href'] if link_el else None
            if link and not link.startswith('http'):
                link = f"https://www.hanwhavision.com{link}"
            
            content_el = item.select_one('p, .description')
            content = content_el.get_text(strip=True) if content_el else title
            
            img_el = item.select_one('img')
            image = img_el.get('src') if img_el else None
            
            if title and len(title) > 10:
                existing = supabase.table("news").select("id").eq("title", title).execute()
                if not existing.data:
                    result = await save_news("Hanwha", title, content or "", image, link)
                    if result:
                        count += 1
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Hanwha: {e}")
    
    logger.info(f"üìä Hanwha: –¥–æ–±–∞–≤–ª–µ–Ω–æ {count} –Ω–æ–≤–æ—Å—Ç–µ–π")
    return count


async def parse_cloudflare():
    """Cloudflare (–µ—Å–ª–∏ –µ—Å—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏)"""
    logger.info("üåê –ü–∞—Ä—Å–∏–Ω–≥ Cloudflare...")
    url = "https://cloudflare.net/news/default.aspx"
    html = await fetch_html(url)
    if not html:
        return 0
    
    soup = BeautifulSoup(html, 'html.parser')
    news_items = soup.select('article, .news-item, .post')[:5]
    
    count = 0
    for item in news_items:
        try:
            title_el = item.select_one('h2, h3, .title, a')
            title = title_el.get_text(strip=True) if title_el else None
            
            link_el = item.select_one('a[href]')
            link = link_el['href'] if link_el else None
            if link and not link.startswith('http'):
                link = f"https://cloudflare.net{link}"
            
            content_el = item.select_one('p, .description')
            content = content_el.get_text(strip=True) if content_el else title
            
            img_el = item.select_one('img')
            image = img_el.get('src') if img_el else None
            
            if title and len(title) > 10:
                existing = supabase.table("news").select("id").eq("title", title).execute()
                if not existing.data:
                    result = await save_news("Cloudflare", title, content or "", image, link)
                    if result:
                        count += 1
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Cloudflare: {e}")
    
    logger.info(f"üìä Cloudflare: –¥–æ–±–∞–≤–ª–µ–Ω–æ {count} –Ω–æ–≤–æ—Å—Ç–µ–π")
    return count


async def parse_rgsec():
    """RGSec"""
    logger.info("üá∑üá∫ –ü–∞—Ä—Å–∏–Ω–≥ RGSec...")
    url = "https://www.rgsec.ru/news"
    html = await fetch_html(url)
    if not html:
        return 0
    
    soup = BeautifulSoup(html, 'html.parser')
    news_items = soup.select('.news-item, article, .post')[:5]
    
    count = 0
    for item in news_items:
        try:
            title_el = item.select_one('h2, h3, .title, a')
            title = title_el.get_text(strip=True) if title_el else None
            
            link_el = item.select_one('a[href]')
            link = link_el['href'] if link_el else None
            if link and not link.startswith('http'):
                link = f"https://www.rgsec.ru{link}"
            
            content_el = item.select_one('p, .description')
            content = content_el.get_text(strip=True) if content_el else title
            
            img_el = item.select_one('img')
            image = img_el.get('src') if img_el else None
            
            if title and len(title) > 10:
                existing = supabase.table("news").select("id").eq("title", title).execute()
                if not existing.data:
                    result = await save_news("RGSec", title, content or "", image, link)
                    if result:
                        count += 1
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ RGSec: {e}")
    
    logger.info(f"üìä RGSec: –¥–æ–±–∞–≤–ª–µ–Ω–æ {count} –Ω–æ–≤–æ—Å—Ç–µ–π")
    return count


async def parse_dssl():
    """DSSL"""
    logger.info("üá∑üá∫ –ü–∞—Ä—Å–∏–Ω–≥ DSSL...")
    url = "https://www.dssl.ru/publications/news/"
    html = await fetch_html(url)
    if not html:
        return 0
    
    soup = BeautifulSoup(html, 'html.parser')
    news_items = soup.select('.news-item, article, .post')[:5]
    
    count = 0
    for item in news_items:
        try:
            title_el = item.select_one('h2, h3, .title, a')
            title = title_el.get_text(strip=True) if title_el else None
            
            link_el = item.select_one('a[href]')
            link = link_el['href'] if link_el else None
            if link and not link.startswith('http'):
                link = f"https://www.dssl.ru{link}"
            
            content_el = item.select_one('p, .description')
            content = content_el.get_text(strip=True) if content_el else title
            
            img_el = item.select_one('img')
            image = img_el.get('src') if img_el else None
            
            if title and len(title) > 10:
                existing = supabase.table("news").select("id").eq("title", title).execute()
                if not existing.data:
                    result = await save_news("DSSL", title, content or "", image, link)
                    if result:
                        count += 1
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ DSSL: {e}")
    
    logger.info(f"üìä DSSL: –¥–æ–±–∞–≤–ª–µ–Ω–æ {count} –Ω–æ–≤–æ—Å—Ç–µ–π")
    return count


async def parse_skud_system():
    """SKUD-System"""
    logger.info("üá∑üá∫ –ü–∞—Ä—Å–∏–Ω–≥ SKUD-System...")
    url = "https://skud-system.ru/news/"
    html = await fetch_html(url)
    if not html:
        return 0
    
    soup = BeautifulSoup(html, 'html.parser')
    news_items = soup.select('.news-item, article, .post')[:5]
    
    count = 0
    for item in news_items:
        try:
            title_el = item.select_one('h2, h3, .title, a')
            title = title_el.get_text(strip=True) if title_el else None
            
            link_el = item.select_one('a[href]')
            link = link_el['href'] if link_el else None
            if link and not link.startswith('http'):
                link = f"https://skud-system.ru{link}"
            
            content_el = item.select_one('p, .description')
            content = content_el.get_text(strip=True) if content_el else title
            
            img_el = item.select_one('img')
            image = img_el.get('src') if img_el else None
            
            if title and len(title) > 10:
                existing = supabase.table("news").select("id").eq("title", title).execute()
                if not existing.data:
                    result = await save_news("SKUD-System", title, content or "", image, link)
                    if result:
                        count += 1
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ SKUD-System: {e}")
    
    logger.info(f"üìä SKUD-System: –¥–æ–±–∞–≤–ª–µ–Ω–æ {count} –Ω–æ–≤–æ—Å—Ç–µ–π")
    return count


# ============ –ö–ê–ó–ê–•–°–¢–ê–ù–°–ö–ò–ï –°–ê–ô–¢–´ ============

async def parse_orion_m2m():
    """Orion M2M"""
    logger.info("üá∞üáø –ü–∞—Ä—Å–∏–Ω–≥ Orion M2M...")
    url = "https://orion-m2m.kz/news/2025/"
    html = await fetch_html(url)
    if not html:
        return 0
    
    soup = BeautifulSoup(html, 'html.parser')
    news_items = soup.select('.news-item, article, .post')[:5]
    
    count = 0
    for item in news_items:
        try:
            title_el = item.select_one('h2, h3, .title, a')
            title = title_el.get_text(strip=True) if title_el else None
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–µ–ª–µ—Ñ–æ–Ω—ã –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
            if not title or len(title) < 15 or title.startswith('+7'):
                continue
            
            link_el = item.select_one('a[href]')
            link = link_el['href'] if link_el else None
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è —Å—Å—ã–ª–∫–∏ - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º tel: –∏ mailto:
            if link:
                if link.startswith('tel:') or link.startswith('mailto:'):
                    continue
                if not link.startswith('http'):
                    link = f"https://orion-m2m.kz{link}"
            
            content_el = item.select_one('p, .description')
            content = content_el.get_text(strip=True) if content_el else title
            
            img_el = item.select_one('img')
            image = img_el.get('src') if img_el else None
            
            if title and len(title) > 10:
                existing = supabase.table("news").select("id").eq("title", title).execute()
                if not existing.data:
                    result = await save_news("Orion M2M", title, content or "", image, link)
                    if result:
                        count += 1
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Orion M2M: {e}")
    
    logger.info(f"üìä Orion M2M: –¥–æ–±–∞–≤–ª–µ–Ω–æ {count} –Ω–æ–≤–æ—Å—Ç–µ–π")
    return count


async def parse_intant():
    """Intant"""
    logger.info("üá∞üáø –ü–∞—Ä—Å–∏–Ω–≥ Intant...")
    url = "https://intant.kz/news/"
    html = await fetch_html(url)
    if not html:
        return 0
    
    soup = BeautifulSoup(html, 'html.parser')
    news_items = soup.select('.news-item, article, .post')[:5]
    
    count = 0
    for item in news_items:
        try:
            title_el = item.select_one('h2, h3, .title, a')
            title = title_el.get_text(strip=True) if title_el else None
            
            link_el = item.select_one('a[href]')
            link = link_el['href'] if link_el else None
            if link and not link.startswith('http'):
                link = f"https://intant.kz{link}"
            
            content_el = item.select_one('p, .description')
            content = content_el.get_text(strip=True) if content_el else title
            
            img_el = item.select_one('img')
            image = img_el.get('src') if img_el else None
            
            if title and len(title) > 10:
                existing = supabase.table("news").select("id").eq("title", title).execute()
                if not existing.data:
                    result = await save_news("Intant", title, content or "", image, link)
                    if result:
                        count += 1
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Intant: {e}")
    
    logger.info(f"üìä Intant: –¥–æ–±–∞–≤–ª–µ–Ω–æ {count} –Ω–æ–≤–æ—Å—Ç–µ–π")
    return count


async def parse_inform_kz():
    """Inform.kz (–≤–∏–¥–µ–æ–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ)"""
    logger.info("üá∞üáø –ü–∞—Ä—Å–∏–Ω–≥ Inform.kz...")
    url = "https://www.inform.kz/tag/videonablyudenie"
    html = await fetch_html(url)
    if not html:
        return 0
    
    soup = BeautifulSoup(html, 'html.parser')
    news_items = soup.select('article, .news-item, .post')[:5]
    
    count = 0
    for item in news_items:
        try:
            title_el = item.select_one('h2, h3, .title, a')
            title = title_el.get_text(strip=True) if title_el else None
            
            link_el = item.select_one('a[href]')
            link = link_el['href'] if link_el else None
            if link and not link.startswith('http'):
                link = f"https://www.inform.kz{link}"
            
            content_el = item.select_one('p, .description, .excerpt')
            content = content_el.get_text(strip=True) if content_el else title
            
            img_el = item.select_one('img')
            image = img_el.get('src') if img_el else None
            
            if title and len(title) > 10:
                existing = supabase.table("news").select("id").eq("title", title).execute()
                if not existing.data:
                    result = await save_news("Inform.kz", title, content or "", image, link)
                    if result:
                        count += 1
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Inform.kz: {e}")
    
    logger.info(f"üìä Inform.kz: –¥–æ–±–∞–≤–ª–µ–Ω–æ {count} –Ω–æ–≤–æ—Å—Ç–µ–π")
    return count


# ============ –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –ü–ê–†–°–ò–ù–ì–ê ============

async def parse_all_sites():
    """–ü–∞—Ä—Å–∏—Ç –≤—Å–µ 21 –∏—Å—Ç–æ—á–Ω–∏–∫"""
    logger.info("üîÑ –ù–∞—á–∏–Ω–∞—é –ø–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö —Å–∞–π—Ç–æ–≤...")
    
    total_count = 0
    
    # –ó–∞—Ä—É–±–µ–∂–Ω—ã–µ
    total_count += await parse_hikvision()
    total_count += await parse_dahua()
    total_count += await parse_axis()
    total_count += await parse_bosch()
    total_count += await parse_siemens()
    total_count += await parse_hochiki()
    total_count += await parse_hanwha()
    total_count += await parse_cloudflare()
    
    # –†–æ—Å—Å–∏–π—Å–∫–∏–µ
    total_count += await parse_bolid()
    total_count += await parse_perco()
    total_count += await parse_rubezh()
    total_count += await parse_rgsec()
    total_count += await parse_dssl()
    total_count += await parse_skud_system()
    
    # –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω—Å–∫–∏–µ
    total_count += await parse_orion_m2m()
    total_count += await parse_intant()
    total_count += await parse_inform_kz()
    
    logger.info(f"‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö —Å–∞–π—Ç–æ–≤ –∑–∞–≤–µ—Ä—à—ë–Ω! –î–æ–±–∞–≤–ª–µ–Ω–æ {total_count} –Ω–æ–≤–æ—Å—Ç–µ–π")


async def scheduled_parsing(context: ContextTypes.DEFAULT_TYPE):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –ø–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é"""
    logger.info("‚è∞ –ó–∞–ø—É—Å–∫ –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞...")
    await parse_all_sites()


# ============ MAIN ============
def main():
    """–ó–∞–ø—É—Å–∫ –±–æ—Ç–∞"""
    app = Application.builder().token(TELEGRAM_TOKEN).build()
    
    # –ö–æ–º–∞–Ω–¥—ã
    app.add_handler(CommandHandler("start", start_command))
    app.add_handler(CommandHandler("parse", parse_command))
    app.add_handler(CommandHandler("list", list_command))
    app.add_handler(CommandHandler("delete", delete_command))
    app.add_handler(CommandHandler("stats", stats_command))
    app.add_handler(CommandHandler("sources", sources_command))
    
    # –û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å–æ–æ–±—â–µ–Ω–∏–π
    app.add_handler(MessageHandler(filters.ALL, handle_message))
    
    # –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ (–∫–∞–∂–¥—ã–µ 2 —á–∞—Å–∞)
    job_queue = app.job_queue
    job_queue.run_repeating(scheduled_parsing, interval=7200, first=10)
    
    logger.info("üöÄ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω!")
    logger.info("üåê –ü–∞—Ä—Å–∏–Ω–≥ 21 –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –æ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏")
    logger.info("üîç –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ ~90 –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º")
    logger.info("üìã –ü–µ—Ä–≤—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ 10 —Å–µ–∫—É–Ω–¥, –∑–∞—Ç–µ–º –∫–∞–∂–¥—ã–µ 2 —á–∞—Å–∞")
    app.run_polling(allowed_updates=Update.ALL_TYPES)


if __name__ == "__main__":
    main()
