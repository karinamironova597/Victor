import asyncio
import logging
from datetime import datetime
from typing import Optional
import aiohttp
from bs4 import BeautifulSoup
from telegram import Update
from telegram.ext import Application, MessageHandler, CommandHandler, filters, ContextTypes
from supabase import create_client, Client

# ============ –ù–ê–°–¢–†–û–ô–ö–ò ============
TELEGRAM_TOKEN = "8540569762:AAFnvJS9v7P6mlfhK1sfGSpQ_nIsY2bbM6s"
SUPABASE_URL = "https://dqpqhvaikapnnmablvxh.supabase.co"
SUPABASE_KEY = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImRxcHFodmFpa2Fwbm5tYWJsdnhoIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc2OTQ5MzE3MywiZXhwIjoyMDg1MDY5MTczfQ.bNsJE5orouvDoCHa4q9p0FwbaMDgsLuhXSIGmN9h7Qc"
CHANNEL_ID = "@iqsafety_news"

# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

# Supabase –∫–ª–∏–µ–Ω—Ç
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)


# ============ –°–û–•–†–ê–ù–ï–ù–ò–ï –í –ë–ê–ó–£ ============
async def save_news(source: str, title: str, content: str, image_url: str = None, post_url: str = None):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–æ–≤–æ—Å—Ç—å –≤ Supabase"""
    try:
        data = {
            "source": source,
            "title": title,
            "content": content,
            "image_url": image_url,
            "post_url": post_url,
            "deleted": False
        }
        result = supabase.table("news").insert(data).execute()
        logger.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {title[:50]}...")
        return result
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")
        return None


# ============ –ö–û–ú–ê–ù–î–´ –ë–û–¢–ê ============
async def start_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–ö–æ–º–∞–Ω–¥–∞ /start"""
    await update.message.reply_text(
        "ü§ñ –ë–æ—Ç –Ω–æ–≤–æ—Å—Ç–µ–π IQ Safety –∑–∞–ø—É—â–µ–Ω!\n\n"
        "üìã –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã:\n"
        "/parse - –ó–∞–ø—É—Å—Ç–∏—Ç—å –ø–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–µ–π –≤—Ä—É—á–Ω—É—é\n"
        "/list - –ü–æ–∫–∞–∑–∞—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ —Å ID\n"
        "/delete <ID> - –£–¥–∞–ª–∏—Ç—å –Ω–æ–≤–æ—Å—Ç—å –ø–æ ID\n"
        "/stats - –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π"
    )


async def parse_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–ö–æ–º–∞–Ω–¥–∞ /parse - –∑–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤—Ä—É—á–Ω—É—é"""
    await update.message.reply_text("üîÑ –ó–∞–ø—É—Å–∫–∞—é –ø–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–µ–π...")
    
    try:
        await parse_all_sites()
        await update.message.reply_text("‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à—ë–Ω! –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö.")
    except Exception as e:
        await update.message.reply_text(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
        logger.error(f"–û—à–∏–±–∫–∞ –≤ parse_command: {e}")


async def list_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–ö–æ–º–∞–Ω–¥–∞ /list - –ø–æ–∫–∞–∑–∞—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏"""
    try:
        response = supabase.table('news').select('id, title, source, deleted').order('created_at', desc=True).limit(15).execute()
        
        if not response.data:
            await update.message.reply_text("üì≠ –ù–æ–≤–æ—Å—Ç–µ–π –ø–æ–∫–∞ –Ω–µ—Ç")
            return
        
        message = "üì∞ –ü–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏:\n\n"
        for news in response.data:
            deleted_mark = " ‚ùå" if news.get('deleted') else ""
            source = news.get('source', 'N/A')
            title = news.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')[:60]
            message += f"ID: {news['id']} | {source}\n{title}...{deleted_mark}\n\n"
        
        await update.message.reply_text(message)
    except Exception as e:
        await update.message.reply_text(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        logger.error(f"–û—à–∏–±–∫–∞ –≤ list_command: {e}")


async def delete_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–ö–æ–º–∞–Ω–¥–∞ /delete <ID> - –ø–æ–º–µ—Ç–∏—Ç—å –Ω–æ–≤–æ—Å—Ç—å –∫–∞–∫ —É–¥–∞–ª—ë–Ω–Ω—É—é"""
    if not context.args:
        await update.message.reply_text("‚ùå –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: /delete <ID –Ω–æ–≤–æ—Å—Ç–∏>\n\n–ü—Ä–∏–º–µ—Ä: /delete 5")
        return
    
    try:
        news_id = int(context.args[0])
        
        response = supabase.table('news').update({
            'deleted': True
        }).eq('id', news_id).execute()
        
        if response.data:
            await update.message.reply_text(f"‚úÖ –ù–æ–≤–æ—Å—Ç—å #{news_id} —É–¥–∞–ª–µ–Ω–∞ –∏–∑ –≤–∏–¥–∂–µ—Ç–∞")
        else:
            await update.message.reply_text(f"‚ùå –ù–æ–≤–æ—Å—Ç—å #{news_id} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
    except ValueError:
        await update.message.reply_text("‚ùå ID –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —á–∏—Å–ª–æ–º!")
    except Exception as e:
        await update.message.reply_text(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        logger.error(f"–û—à–∏–±–∫–∞ –≤ delete_command: {e}")


async def stats_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–ö–æ–º–∞–Ω–¥–∞ /stats - —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π"""
    try:
        all_news = supabase.table('news').select('id, source, deleted').execute()
        
        total = len(all_news.data)
        deleted = len([n for n in all_news.data if n.get('deleted')])
        active = total - deleted
        
        sources = {}
        for news in all_news.data:
            source = news.get('source', 'Unknown')
            sources[source] = sources.get(source, 0) + 1
        
        message = f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π:\n\n"
        message += f"üì∞ –í—Å–µ–≥–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {total}\n"
        message += f"‚úÖ –ê–∫—Ç–∏–≤–Ω—ã—Ö: {active}\n"
        message += f"‚ùå –£–¥–∞–ª—ë–Ω–Ω—ã—Ö: {deleted}\n\n"
        message += "üìç –ü–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º:\n"
        for source, count in sorted(sources.items(), key=lambda x: x[1], reverse=True):
            message += f"  ‚Ä¢ {source}: {count}\n"
        
        await update.message.reply_text(message)
    except Exception as e:
        await update.message.reply_text(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        logger.error(f"–û—à–∏–±–∫–∞ –≤ stats_command: {e}")


# ============ TELEGRAM HANDLER ============
async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ –≤—Ö–æ–¥—è—â–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è"""
    message = update.message or update.channel_post
    if not message:
        return
    
    text = message.text or message.caption or ""
    
    if text.startswith('/'):
        return
    
    image_url = None
    if message.photo:
        photo = message.photo[-1]
        file = await context.bot.get_file(photo.file_id)
        image_url = file.file_path
    
    post_url = None
    if update.channel_post:
        post_url = f"https://t.me/{CHANNEL_ID.replace('@', '')}/{message.message_id}"
    
    title = text.split('\n')[0][:100] if text else "–ù–æ–≤–æ—Å—Ç—å IQ Safety"
    
    result = await save_news(
        source="IQ Safety",
        title=title,
        content=text,
        image_url=image_url,
        post_url=post_url
    )
    
    if update.message and result:
        try:
            if image_url:
                await context.bot.send_photo(
                    chat_id=CHANNEL_ID,
                    photo=image_url,
                    caption=text
                )
            else:
                await context.bot.send_message(
                    chat_id=CHANNEL_ID,
                    text=text
                )
            await update.message.reply_text("‚úÖ –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ –≤ –∫–∞–Ω–∞–ª–µ!")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –≤ –∫–∞–Ω–∞–ª: {e}")
            await update.message.reply_text(f"‚ùå –û—à–∏–±–∫–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {e}")


# ============ –ü–ê–†–°–ò–ù–ì –°–ê–ô–¢–û–í ============
async def fetch_html(url: str) -> Optional[str]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        async with aiohttp.ClientSession() as session:
            async with session.get(url, timeout=30, headers=headers) as response:
                if response.status == 200:
                    return await response.text()
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}: {e}")
    return None


async def parse_tengrinews():
    """–ü–∞—Ä—Å–∏—Ç Tengrinews.kz - –≥–ª–∞–≤–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω–∞"""
    logger.info("üá∞üáø –ü–∞—Ä—Å–∏–Ω–≥ Tengrinews.kz...")
    url = "https://tengrinews.kz/"
    html = await fetch_html(url)
    if not html:
        return
    
    soup = BeautifulSoup(html, 'html.parser')
    news_items = soup.select('.main-news_top_item, .main-news_content_item')[:5]
    
    count = 0
    for item in news_items:
        try:
            title_el = item.select_one('.main-news_top_item_title, .main-news_content_item_title, a')
            title = title_el.get_text(strip=True) if title_el else None
            
            link_el = item.select_one('a[href]')
            link = link_el['href'] if link_el else None
            if link and not link.startswith('http'):
                link = f"https://tengrinews.kz{link}"
            
            img_el = item.select_one('img')
            image = img_el.get('src') or img_el.get('data-src') if img_el else None
            if image and not image.startswith('http'):
                image = f"https://tengrinews.kz{image}"
            
            if title and len(title) > 10:
                existing = supabase.table("news").select("id").eq("title", title).execute()
                if not existing.data:
                    await save_news("Tengrinews", title, title, image, link)
                    count += 1
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Tengrinews: {e}")
    
    logger.info(f"üìä Tengrinews: –¥–æ–±–∞–≤–ª–µ–Ω–æ {count} –Ω–æ–≤–æ—Å—Ç–µ–π")


async def parse_forbes_kz():
    """–ü–∞—Ä—Å–∏—Ç Forbes.kz - –±–∏–∑–Ω–µ—Å –Ω–æ–≤–æ—Å—Ç–∏"""
    logger.info("üíº –ü–∞—Ä—Å–∏–Ω–≥ Forbes.kz...")
    url = "https://forbes.kz/"
    html = await fetch_html(url)
    if not html:
        return
    
    soup = BeautifulSoup(html, 'html.parser')
    news_items = soup.select('article, .article-item, .news-item')[:5]
    
    count = 0
    for item in news_items:
        try:
            title_el = item.select_one('h2, h3, .article-title, a')
            title = title_el.get_text(strip=True) if title_el else None
            
            link_el = item.select_one('a[href]')
            link = link_el['href'] if link_el else None
            if link and not link.startswith('http'):
                link = f"https://forbes.kz{link}"
            
            content_el = item.select_one('p, .excerpt, .description')
            content = content_el.get_text(strip=True) if content_el else title
            
            img_el = item.select_one('img')
            image = img_el.get('src') or img_el.get('data-src') if img_el else None
            
            if title and len(title) > 10:
                existing = supabase.table("news").select("id").eq("title", title).execute()
                if not existing.data:
                    await save_news("Forbes KZ", title, content, image, link)
                    count += 1
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Forbes: {e}")
    
    logger.info(f"üìä Forbes KZ: –¥–æ–±–∞–≤–ª–µ–Ω–æ {count} –Ω–æ–≤–æ—Å—Ç–µ–π")


async def parse_zakon_kz():
    """–ü–∞—Ä—Å–∏—Ç Zakon.kz - –ø—Ä–∞–≤–æ–≤–æ–π –ø–æ—Ä—Ç–∞–ª"""
    logger.info("‚öñÔ∏è –ü–∞—Ä—Å–∏–Ω–≥ Zakon.kz...")
    url = "https://www.zakon.kz/news"
    html = await fetch_html(url)
    if not html:
        return
    
    soup = BeautifulSoup(html, 'html.parser')
    news_items = soup.select('.news-item, article, .post-item')[:5]
    
    count = 0
    for item in news_items:
        try:
            title_el = item.select_one('h2, h3, .news-title, a')
            title = title_el.get_text(strip=True) if title_el else None
            
            link_el = item.select_one('a[href]')
            link = link_el['href'] if link_el else None
            if link and not link.startswith('http'):
                link = f"https://www.zakon.kz{link}"
            
            content_el = item.select_one('p, .news-excerpt')
            content = content_el.get_text(strip=True) if content_el else title
            
            img_el = item.select_one('img')
            image = img_el.get('src') if img_el else None
            
            if title and len(title) > 10:
                existing = supabase.table("news").select("id").eq("title", title).execute()
                if not existing.data:
                    await save_news("Zakon.kz", title, content, image, link)
                    count += 1
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Zakon: {e}")
    
    logger.info(f"üìä Zakon.kz: –¥–æ–±–∞–≤–ª–µ–Ω–æ {count} –Ω–æ–≤–æ—Å—Ç–µ–π")


async def parse_inbusiness():
    """–ü–∞—Ä—Å–∏—Ç InBusiness.kz - –¥–µ–ª–æ–≤–æ–π –ø–æ—Ä—Ç–∞–ª"""
    logger.info("üí∞ –ü–∞—Ä—Å–∏–Ω–≥ InBusiness.kz...")
    url = "https://inbusiness.kz/ru/news"
    html = await fetch_html(url)
    if not html:
        return
    
    soup = BeautifulSoup(html, 'html.parser')
    news_items = soup.select('.news-list-item, article, .news-item')[:5]
    
    count = 0
    for item in news_items:
        try:
            title_el = item.select_one('h2, h3, .title, a')
            title = title_el.get_text(strip=True) if title_el else None
            
            link_el = item.select_one('a[href]')
            link = link_el['href'] if link_el else None
            if link and not link.startswith('http'):
                link = f"https://inbusiness.kz{link}"
            
            content_el = item.select_one('p, .description')
            content = content_el.get_text(strip=True) if content_el else title
            
            img_el = item.select_one('img')
            image = img_el.get('src') if img_el else None
            
            if title and len(title) > 10:
                existing = supabase.table("news").select("id").eq("title", title).execute()
                if not existing.data:
                    await save_news("InBusiness", title, content, image, link)
                    count += 1
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ InBusiness: {e}")
    
    logger.info(f"üìä InBusiness: –¥–æ–±–∞–≤–ª–µ–Ω–æ {count} –Ω–æ–≤–æ—Å—Ç–µ–π")


async def parse_kapital_kz():
    """–ü–∞—Ä—Å–∏—Ç Kapital.kz - —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–π –ø–æ—Ä—Ç–∞–ª"""
    logger.info("üìà –ü–∞—Ä—Å–∏–Ω–≥ Kapital.kz...")
    url = "https://kapital.kz/economic"
    html = await fetch_html(url)
    if not html:
        return
    
    soup = BeautifulSoup(html, 'html.parser')
    news_items = soup.select('article, .news-item, .post')[:5]
    
    count = 0
    for item in news_items:
        try:
            title_el = item.select_one('h2, h3, .title, a')
            title = title_el.get_text(strip=True) if title_el else None
            
            link_el = item.select_one('a[href]')
            link = link_el['href'] if link_el else None
            if link and not link.startswith('http'):
                link = f"https://kapital.kz{link}"
            
            content_el = item.select_one('p, .description')
            content = content_el.get_text(strip=True) if content_el else title
            
            img_el = item.select_one('img')
            image = img_el.get('src') or img_el.get('data-src') if img_el else None
            
            if title and len(title) > 10:
                existing = supabase.table("news").select("id").eq("title", title).execute()
                if not existing.data:
                    await save_news("Kapital.kz", title, content, image, link)
                    count += 1
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Kapital: {e}")
    
    logger.info(f"üìä Kapital.kz: –¥–æ–±–∞–≤–ª–µ–Ω–æ {count} –Ω–æ–≤–æ—Å—Ç–µ–π")


async def parse_all_sites():
    """–ü–∞—Ä—Å–∏—Ç –≤—Å–µ —Å–∞–π—Ç—ã"""
    logger.info("üîÑ –ù–∞—á–∏–Ω–∞—é –ø–∞—Ä—Å–∏–Ω–≥ –∫–∞–∑–∞—Ö—Å—Ç–∞–Ω—Å–∫–∏—Ö —Å–∞–π—Ç–æ–≤...")
    
    await parse_tengrinews()
    await parse_forbes_kz()
    await parse_zakon_kz()
    await parse_inbusiness()
    await parse_kapital_kz()
    
    logger.info("‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö —Å–∞–π—Ç–æ–≤ –∑–∞–≤–µ—Ä—à—ë–Ω")


async def scheduled_parsing(context: ContextTypes.DEFAULT_TYPE):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –ø–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é"""
    logger.info("‚è∞ –ó–∞–ø—É—Å–∫ –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞...")
    await parse_all_sites()


# ============ MAIN ============
def main():
    """–ó–∞–ø—É—Å–∫ –±–æ—Ç–∞"""
    app = Application.builder().token(TELEGRAM_TOKEN).build()
    
    # –ö–æ–º–∞–Ω–¥—ã
    app.add_handler(CommandHandler("start", start_command))
    app.add_handler(CommandHandler("parse", parse_command))
    app.add_handler(CommandHandler("list", list_command))
    app.add_handler(CommandHandler("delete", delete_command))
    app.add_handler(CommandHandler("stats", stats_command))
    
    # –û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å–æ–æ–±—â–µ–Ω–∏–π
    app.add_handler(MessageHandler(filters.ALL, handle_message))
    
    # –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ (–∫–∞–∂–¥—ã–µ 2 —á–∞—Å–∞)
    job_queue = app.job_queue
    job_queue.run_repeating(scheduled_parsing, interval=7200, first=10)
    
    logger.info("üöÄ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω!")
    logger.info("üá∞üáø –ü–∞—Ä—Å–∏–Ω–≥ –∫–∞–∑–∞—Ö—Å—Ç–∞–Ω—Å–∫–∏—Ö –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Å–∞–π—Ç–æ–≤")
    logger.info("üìã –ü–µ—Ä–≤—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ 10 —Å–µ–∫—É–Ω–¥, –∑–∞—Ç–µ–º –∫–∞–∂–¥—ã–µ 2 —á–∞—Å–∞")
    app.run_polling(allowed_updates=Update.ALL_TYPES)


if __name__ == "__main__":
    main()
